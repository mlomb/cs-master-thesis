{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from lib.service import SamplesService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_int64_bitset(x):\n",
    "    \"\"\"\n",
    "    Convert a 64-bit integer into a 64-element float tensor\n",
    "    \"\"\"\n",
    "    masks = 2 ** torch.arange(64, dtype=torch.int64, device='cuda')\n",
    "    expanded = torch.bitwise_and(x.unsqueeze(-1), masks).ne(0).to(torch.float32)\n",
    "    return expanded\n",
    "\n",
    "intermediate_scale = 1 / 64\n",
    "output_scale = 1 / 9600\n",
    "\n",
    "class ChessModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(ChessModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(num_features, 1024)\n",
    "        self.linear2 = torch.nn.Linear(1024, 64)\n",
    "        self.linear3 = torch.nn.Linear(64, 64)\n",
    "        self.output = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0) # Clipped ReLU\n",
    "        x = self.linear2(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0) # Clipped ReLU\n",
    "        x = self.linear3(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0) # Clipped ReLU\n",
    "        return self.output(x)\n",
    "    \n",
    "    def _clip_weights(self):\n",
    "        intermediate_clip = 127 * intermediate_scale\n",
    "        output_clip = 127*127 * output_scale\n",
    "\n",
    "        self.linear1.weight.data.clamp_(-intermediate_clip, intermediate_clip)\n",
    "        self.linear2.weight.data.clamp_(-intermediate_clip, intermediate_clip)\n",
    "        self.linear3.weight.data.clamp_(-intermediate_clip, intermediate_clip)\n",
    "        self.output.weight.data.clamp_(-output_clip, output_clip)\n",
    "\n",
    "    def _layer_json(self, layer: torch.nn.Linear, scale_value: float, activation: str | None = None):\n",
    "        weights = layer.weight.data.cpu().numpy()\n",
    "        bias = layer.bias.data.cpu().numpy()\n",
    "\n",
    "        qweights = np.round(weights / scale_value).astype('int8')\n",
    "        qbias = np.round(bias / (1 / 32_)).astype('int32')\n",
    "\n",
    "        obj = {\n",
    "            \"num_inputs\": layer.in_features,\n",
    "            \"num_outputs\": layer.out_features,\n",
    "            \"weights\": qweights.reshape(-1).tolist(),\n",
    "            \"bias\": qbias.reshape(-1).tolist()\n",
    "        }\n",
    "\n",
    "        if activation is not None:\n",
    "            obj[\"activation\"] = activation\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"\n",
    "        Returns the layers of the model as a list of dictionaries\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        layers.append(self._layer_json(self.linear1, intermediate_scale, activation='clip'))\n",
    "        layers.append(self._layer_json(self.linear2, intermediate_scale, activation='clip'))\n",
    "        layers.append(self._layer_json(self.linear3, intermediate_scale, activation='clip'))\n",
    "        layers.append(self._layer_json(self.output, output_scale))\n",
    "        return layers\n",
    "\n",
    "#testmodel = ChessModel(768)\n",
    "#testmodel.load_state_dict(torch.load('/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/notebooks/runs/20240308_130550_eval_basic_4096/models/400.pth'))\n",
    "#testmodel.cuda()\n",
    "\n",
    "#X, y = samples_service.next_batch()\n",
    "#X = decode_int64_bitset(X)\n",
    "#X = X.reshape(-1, NUM_FEATURES)\n",
    "#print(testmodel(X) / 356 * 100, y)\n",
    "\n",
    "#import json\n",
    "#with open('model.json', 'w') as f:\n",
    "#    f.write(json.dumps(testmodel.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PQRLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PQRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred):\n",
    "        pred = pred.reshape(-1, 3)\n",
    "        \n",
    "        p = pred[:,0]\n",
    "        q = pred[:,1]\n",
    "        r = pred[:,2]\n",
    "        \n",
    "        a = -torch.mean(torch.log(torch.sigmoid(r - q)))\n",
    "        b = torch.mean(torch.square(p + q))\n",
    "\n",
    "        loss = a + b\n",
    "\n",
    "        return loss\n",
    "\n",
    "class EvalLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # since we are clipping the layers, we need to scale the output so it can reach higher values\n",
    "        output = output * 600.0\n",
    "\n",
    "        scaling = 356.0\n",
    "\n",
    "        # scale CP score to engine units [-10_000, 10_000]\n",
    "        target = target * scaling / 100.0\n",
    "\n",
    "        # targets are in CP-space change it to WDL-space [0, 1]\n",
    "        wdl_model = torch.sigmoid(output / scaling)\n",
    "        wdl_target = torch.sigmoid(target / scaling)\n",
    "\n",
    "        loss = torch.pow(torch.abs(wdl_model - wdl_target), 2.5)\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-10 22:06:22.432864: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-10 22:06:22.433007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-10 22:06:22.518002: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-10 22:06:22.681179: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-10 22:06:24.737950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Epoch 0: 100%|██████████| 1000/1000 [00:12<00:00, 79.59it/s]\n",
      "Epoch 1:   6%|▌         | 60/1000 [00:00<00:10, 89.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m X \u001b[38;5;241m=\u001b[39m decode_int64_bitset(X)\n\u001b[1;32m     71\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, NUM_FEATURES)\n\u001b[0;32m---> 73\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(avg_loss):\n",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mchessmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mChessModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(x)\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(x, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m# Clipped ReLU\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(x, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m# Clipped ReLU\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "EPOCHS = 100000\n",
    "BATCHES_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "FEATURE_SET = \"basic\"\n",
    "NUM_FEATURES = 768\n",
    "METHOD = \"eval\"\n",
    "\n",
    "if METHOD == \"pqr\":\n",
    "    X_SHAPE = (BATCH_SIZE, 3, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 0)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/pqr-1700/*.csv\")\n",
    "    loss_fn = PQRLoss()\n",
    "elif METHOD == \"eval\":\n",
    "    X_SHAPE = (BATCH_SIZE, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 1)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/eval/*.csv\")\n",
    "    loss_fn = EvalLoss()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "folder = f'runs/{timestamp}_{METHOD}_{FEATURE_SET}_{BATCH_SIZE}'\n",
    "os.makedirs(f'{folder}/models', exist_ok=True)\n",
    "\n",
    "samples_service = SamplesService(x_shape=X_SHAPE, y_shape=Y_SHAPE, inputs=INPUTS, feature_set=FEATURE_SET, method=METHOD)\n",
    "chessmodel = ChessModel(num_features=NUM_FEATURES)\n",
    "chessmodel.cuda()\n",
    "\n",
    "#for i in tqdm(range(1000000)):\n",
    "#    a = samples_service.next_batch()\n",
    "\n",
    "optimizer = torch.optim.Adam(chessmodel.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.0001, factor=0.7, patience=10)\n",
    "writer = SummaryWriter(folder)\n",
    "\n",
    "# @torch.compile # 30% speedup\n",
    "def train_step(X, y):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = chessmodel(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    chessmodel._clip_weights()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Make sure gradient tracking is on\n",
    "chessmodel.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _ in tqdm(range(BATCHES_PER_EPOCH), desc=f'Epoch {epoch}'):\n",
    "        X, y = samples_service.next_batch()\n",
    "    \n",
    "        # expand bitset\n",
    "        X = decode_int64_bitset(X)\n",
    "        X = X.reshape(-1, NUM_FEATURES)\n",
    "\n",
    "        loss = train_step(X, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        if math.isnan(avg_loss):\n",
    "            raise Exception(\"Loss is NaN, exiting\")\n",
    "\n",
    "    avg_loss /= BATCHES_PER_EPOCH\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    writer.add_scalar('Train/loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train/lr', scheduler._last_lr[0], epoch) # get_last_lr()\n",
    "    writer.add_scalar('Params/mean-l1', torch.mean(chessmodel.linear1.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l2', torch.mean(chessmodel.linear2.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l3', torch.mean(chessmodel.linear3.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l4', torch.mean(chessmodel.output.weight), epoch)\n",
    "    for name, param in chessmodel.named_parameters():\n",
    "        writer.add_histogram(name, param, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # save model\n",
    "    model_path = f'{folder}/models/{epoch}'\n",
    "    torch.save(chessmodel.state_dict(), f'{model_path}.pth')\n",
    "    with open(f'{model_path}.json', 'w') as f:\n",
    "        model = {\n",
    "            \"config\": {\n",
    "                \"batches_per_epoch\": BATCHES_PER_EPOCH,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "\n",
    "                \"feature_set\": FEATURE_SET,\n",
    "                \"num_features\": NUM_FEATURES,\n",
    "                \"method\": METHOD,\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": avg_loss\n",
    "            },\n",
    "            \"layers\": chessmodel.to_json()\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(chessmodel.output.parameters())[0].cpu().detach().numpy()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# plot distribution\n",
    "sns.histplot(a.flatten(), kde=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  69.   -3.  -43.  112.   76.  104.  244.   74. -157.  367.  139.   79.\n",
      "   36.  -73.  -90.  -24. -132.  201.  144.  -84. -111.  239.   96.  496.\n",
      "  347.   19.   61.  -10.  236.  225.  175.   91.  287.  366.  322.   23.\n",
      "    7.  372. -213.  -47.   21.  322.  271.  144.  294. -154. -190.   79.\n",
      "   47. -273.  160.   66.   11.   94.  196.  378. -135. -216.  226. -232.\n",
      "   31. -166.  -49.  -91.    5. -143.   82.   66.    3.  205.   89.  -66.\n",
      "  164.   -7.   98.  261.  172. -207.  -35.   97.  312.   33.  294. -146.\n",
      "  -58.   61.  285.  200.   30.   24.   76. -128.  214.   58.  198.   33.\n",
      "   -1.  217.  196.  138.  -50.  326.  250.  261.  299.  -61.  183.   55.\n",
      "   40.   -5.  306.  163.  325. -109. -111. -189.  345.  195. -226.  192.\n",
      " -162.   95.  219.  310.  325.  143.  145.  170.  170. -142.  362.  260.\n",
      "  301.  174.  291.  222. -119.  271.  110.  -61. -174.  415.  241.   95.\n",
      "   79. -155.  -10.  187.   63.   -8.  125.  130.  117.  373.  -95.  215.\n",
      "   12.  118.  124.   19.    1.  400.  -37.  -82.  149.  -98.   28.  260.\n",
      "  -61.  106.  196.  309. -154.  304. -159. -199.  202.  269.  247.  378.\n",
      "  399.  103.  304. -122. -213.  208.  -74.  420.  304.  -55.  381.  134.\n",
      "  282. -213.  218.   61. -200. -108.  -99.  -31.  262.   39. -204.   64.\n",
      "  270.  272.  209.  250.  180.  239.  -74.  132.  131.   15.  -22.  235.\n",
      "  148.  330.  -15.   -4. -131.  166.   32. -181.   17.  -81.   53.  -26.\n",
      "   98.   12.  129.   70.  327.  252.  299.  167.  350.   86.  -13.  287.\n",
      "   12.   10.  161.  107.  229. -161. -126.  181.   16.   61.   84.  108.\n",
      "  140.  157.  281.  174.   65. -189.  259.  330.   63.  231. -132. -100.\n",
      "   56.  124. -107. -233.  199.  -79.  291.  115.  265. -172.  278.  156.\n",
      "  105.  319.  -65.  -87.  223.  292.  -77.  212.  224.  154.  113.  -38.\n",
      "  -62.  -90.    8.  298.  142.  146.  -55. -213.  -72. -107.  -97.   18.\n",
      "  136.  -33. -105.   -4.  -15. -146.  170.  330.  -74. -186.  244.  354.\n",
      "  192.  159.  234.  239.  353.   78.  389. -159. -169.  377.  263.  -68.\n",
      "  -99.  132.  360.  -45.  139.  369.   80.  -22.  135.   27.  312.  290.\n",
      "  -28.  249.  128. -133.  160. -185.   95.  230.   59.  275.   19.  320.\n",
      "  136.  -44.   32.  195.  -67.  256.  179. -192.  189. -137.  257.  -46.\n",
      " -145.  -27.  210.  -17. -205.   49.  265.  157.  276. -151.   96.  -94.\n",
      "  285.  187.   -3.  146.  159.  -96.  120. -186.  370.  -14.  373.  -12.\n",
      "  371.  -46.  282. -239.  -25.  -14.  -40.  224.   88.   58.  253.  304.\n",
      "  191.  -74.   69.  157.  -84.  308.  104. -199. -235.  -42.  -41. -316.\n",
      "   77.  164.  278.   43. -118.   20.  134.  -57.  -63.  236.   26.  244.\n",
      "  121. -131.   32.   12.  282.  106.   -1.  -58.  226. -145.  214.   -8.\n",
      "    7.  236.  130. -120.  306.  -29.  -10.  134.  -94.   46.  101.  241.\n",
      " -168.   -5.  365.  171.  225.  154.  304.  -89.   68.  -92.  387.  216.\n",
      "  268.  302.  159.   49.   -3.  187.   88.  -89.  374.  199.  271.  327.\n",
      "  -18.  212.  267.  176.  193.  154.   33.  342.   31.   97.  259.  376.\n",
      "  361.  -82. -247.  435.  166.  263.  137. -167.  317.  275. -132.  317.\n",
      "  219.  -36.  -78.  -12.  147. -134.  183.  166.  147. -125. -165. -134.\n",
      " -120.  244.  173.  196.  390.  257.   35.   64. -149.  -92.  301.  338.\n",
      "  -66. -231. -166.   17.  310.  227.   31. -239.  400.   89.  -37. -139.\n",
      "  -11.  154.  275. -160.  391.  148.  207.   51.  112. -121. -229. -155.\n",
      "   81. -259.  231.  124. -114.  128.  162.  115.  127.   41.   15.  -41.\n",
      "   51.   42.  225. -113.  324.   33. -110.  322.  200. -180.  301.   86.\n",
      "  103.   58.   70.   89.   93.  344.  185.  -43.  344.  -36.  345.  142.\n",
      "  -29.  284.  313.  249. -162.    9.  237.   74.  174.  172.  -60.  210.\n",
      "  334. -267. -129. -167. -160.   66. -105.  377.   51.   16.  -89.   86.\n",
      "  230.  154.  301. -164. -109.  231. -174.   55.  284.  203.  240.   76.\n",
      " -126.  368.   60.   91.   88.    6.  328.  119.  117.  274.  312.   24.\n",
      "  134.   27.   85.    2. -127. -159.  144.   92.  383.   91.   42. -113.\n",
      "  167. -204.  298.  315.  212. -123.  258.  150.  -69.  180.  -34.  -56.\n",
      "  165.   65.  110.  369.  349.  114.   14.   87. -216. -185.  -39.   50.\n",
      "  171.  216.   78.  191.  376.  278. -117.  238.  418.  115.  120.  161.\n",
      "  162. -175.  300.  104. -140.   71.  227.  135.   72.   87.  313.  118.\n",
      "   48.  200.  297.   79.  -10.  -35.  373.  189.   -8.   55.  -14.  260.\n",
      "   80.  176. -179.   51.  212.  -19.  -84.   99.  305.  296.  208.  146.\n",
      "   -1. -184.  292.  148.  283.  133.  385.  -24.   -9.  266.   77.   54.\n",
      " -157.  295.  110.  204.  200.   31.  237.   -8.  102. -175.  184.  127.\n",
      "  -82.  136.  409.  185.  233.  168.   79.  262.   91.   87.  170.  -27.\n",
      " -225.  -76.   20.  184.  198.  328.  195.  203. -192.  160.  354.   86.\n",
      "  308.  209.  121.  294.  140.  250.  -55.   81.  453.  -34.  183.  362.\n",
      " -128.   59.  386.  388.  112.   89.  215. -174.  128.  150.   12.  228.\n",
      "  -60. -155.  274.  319.  249. -136.  -72.  166.  324.   53. -144.  293.\n",
      "  133.  325.  308.  400.   11.  -65.  -85.   70.  258.  291. -224.  139.\n",
      "  -45.   95.  281.   29.  -55.  296.  305.  -57.  150.  -64.   60. -151.\n",
      "  180.  184.  246.  -52. -129.  139.  290.   64.   99.   50.  242.   28.\n",
      "  357.  -12.   57.  186.  149.   93.  249.  124.  176.  -17.  192.  -17.\n",
      "  311.  -51.   49.  -49.  -59. -123.  166. -162.  213. -119.  379.  226.\n",
      "  133.  286.  176.   19.  242.  491.  236.  322. -159.   64.  208.  128.\n",
      " -125.  400.   -6.  354.   36. -211.    1.  266.  157.    5.  -86.  -96.\n",
      " -126.   22.  271. -248.    2.   41.  230.  144.   79. -145.  -94.  -84.\n",
      "  280.  186.  185. -134.   40.  -53. -111.  -55.  154.  108. -136. -181.\n",
      "  236.  189.  383.  348.  380.  159. -149.  -94.  288. -129.  320.  117.\n",
      " -143. -100.  393.   57.  -21.  118.  101.  -42.   35.   24.  340.  -75.\n",
      " -177.  114.  375.  270.  -90.   72.  250. -181.   46.  120.  146.  -83.\n",
      "  -80.  182.  -81.  353. -173.  122.   64. -119.  345.  235.  -66.  -72.\n",
      "  -41.  277. -147. -173.  -92.   38.  340.  223.  154.   75.   78.  269.\n",
      "  279.  123.  183. -118.  333.  169.  443.  244.   99.  114.   24.  245.\n",
      "  356.   81.   77.  -11.  -36.  168.  154.  305.  281.   75.  -62.  113.\n",
      " -104.  225.  368.  192. -135.   59.   98.   -6.  111.  234.  308.  196.\n",
      "  -57.  348.   90.  -91.  -79.  145. -143.  258.  206.  414.  -72. -145.\n",
      " -101.   93. -163.  291.  -43.  101. -106.  263. -190.  147.  -76.  -44.\n",
      "  -50.  166.  -32.  243.]\n",
      "[[  6.087655   -16.759563    -0.03683725  -2.9541297   -1.9835813\n",
      "   -6.33653      0.7038567   19.543407    -1.1153032   18.417328\n",
      "   -3.253191    18.057617    -0.91253686  17.674953     9.373629\n",
      "   14.900639    19.67012     18.323195    11.98364     14.6885395\n",
      "  -14.543674    -3.4043233    1.0513098   34.599167    25.404251\n",
      "    3.6431868   -0.5518178   19.28945     16.696293     6.803376\n",
      "    9.524029    -4.095558    31.345325    18.660423    -3.6011968\n",
      "   -3.5776541   -7.6884007    8.712947    -8.558988    -8.809278\n",
      "    8.104976    16.105167     0.13527386  -8.055515    13.000038\n",
      "    5.510936     3.8212113   24.875772    -3.7738197  -27.573902\n",
      "   14.867102    25.61523     -4.373621    -7.3630853    2.5208764\n",
      "    6.802144     3.050794   -15.108291    14.373539    13.078235\n",
      "   22.413387    12.611219    21.767977     1.8748859    2.2731807\n",
      "    9.493959    14.657556    -2.3569167    1.0690109    2.1981642\n",
      "  -14.674671    -6.0426445   27.265326    18.373178    -0.37938672\n",
      "    2.7989256   24.30053     -0.10326989  21.30521      8.642552\n",
      "   19.02506     -5.2702403   30.16956    -23.83023      0.73203486\n",
      "    0.3101184   27.325111    17.284555     6.7846017  -10.708133\n",
      "   17.448765     2.3178356   24.43078     -4.0864763    0.6736081\n",
      "   -5.3660474    7.0537024  -13.249116     1.0925025   31.791836\n",
      "    6.0481873   19.36914     19.151735    32.198383     2.6375358\n",
      "   -4.2196994   -0.64958876  -1.2833382   36.802975     2.4631727\n",
      "   41.36221      9.223122     7.2593226    6.9678226  -10.744715\n",
      "    7.7695093   13.491607     6.830764    -7.566525    13.201796\n",
      "    2.286301   -31.8046      13.484576   -18.096136    16.210798\n",
      "   15.84901      4.27548     19.674679    13.681227     4.6491647\n",
      "   12.473954    22.88707      3.9793723   13.441364    -9.082649\n",
      "   13.852471    15.169275    32.502983     4.5083275   21.65353\n",
      "   -7.4675484   29.183828    18.211655     3.079318    19.83256\n",
      "   -4.3221755   10.647412    10.21461     14.768171    -0.20059003\n",
      "   -1.4382509   -8.595319     7.9872003    8.599045     9.050063\n",
      "   -5.71766     -4.574848    22.75132     -8.788871    11.6215105\n",
      "   12.63518     23.432789    12.44653     32.696323    23.519365\n",
      "   -2.1154635    2.9768798   27.626251    -1.507067     7.469216\n",
      "   20.205212     6.750728     6.087712    22.980791    22.74591\n",
      "   -7.010522    -9.489562    19.431349    16.973        7.693471\n",
      "   23.87398      6.932978     6.5194926   11.258033    23.339537\n",
      "    6.3546395   21.001682    -7.371938     7.253958    -0.85580236\n",
      "    9.27106     31.02756      2.9709709   -1.3745353    1.3828969\n",
      "   -7.84043     -5.657806    14.656599     8.576072    16.821\n",
      "  -10.175046    17.881601    -0.04886538  -8.741198    16.818369\n",
      "    6.314504     4.188901    -3.4044068   14.510916    22.767162\n",
      "  -21.261019    17.726458    28.668112    21.0093      23.484629\n",
      "   18.295025    13.020745     5.2139854   16.690393    10.219208\n",
      "   -3.8344712   16.989145    -0.54199016 -14.343574     7.846051\n",
      "    4.221893     0.8696692   28.69629     21.507645    13.056564\n",
      "  -18.411308    -0.42738122   2.6471932   -3.8019702    2.1464043\n",
      "   -4.68138     14.86529     10.447239   -24.574072    15.536967\n",
      "    0.56218004  28.195953     6.775778    10.447815     0.05981978\n",
      "    1.7796023   -7.109759     9.685583     1.8753428   -6.25426\n",
      "   20.203903    13.40068      2.8373034   19.4219       6.3397026\n",
      "    4.879348    -6.175215    11.776389   -13.097411    -1.9728411\n",
      "   12.496205     7.8442545    4.830485    13.493295    -7.345786\n",
      "   -2.602267     6.4672565   13.521923    -5.5760016    0.39859748\n",
      "   15.666749     0.8898154    4.1057043   15.943021    12.990382\n",
      "   -0.03738347   6.1041107   -2.397926     2.283595   -12.3503275\n",
      "   16.384361     1.9938618   27.443874    11.265129     1.9243885\n",
      "   15.468907    -3.737649    13.110858    20.253122     3.6804228\n",
      "   12.090065     0.23880638   6.7556753   30.242744     0.19062608\n",
      "  -31.004793   -19.204937     5.195583   -17.673183    -5.440864\n",
      "   12.743185    -1.075821    14.743725     4.540933     5.8262773\n",
      "   -4.31113     33.56821     15.030418   -16.758663    -1.7939522\n",
      "   30.250406    28.722412     4.621043    18.935116    12.942907\n",
      "   15.981111    11.116806    -3.8330145    1.7196178  -15.580069\n",
      "   30.620989    -0.04406683   5.1930795   19.129776    -2.8951552\n",
      "    5.943341     3.8877568   20.832209    10.669792    21.121655\n",
      "   -7.5969315  -13.109633     4.9839125    0.61033803   1.9833312\n",
      "   23.06271      7.592666   -12.226783     9.690003    20.733881\n",
      "    2.1775265    6.6852703  -27.214428    19.106215    11.581284\n",
      "    8.153279     5.6502366   15.95875     -8.963501     6.5225325\n",
      "   19.885452    -2.4513144    8.943789    12.712625    14.019981\n",
      "   17.193687     2.7161682   -5.4778814    7.7550783   23.825764\n",
      "   -5.261877    20.891197    -3.9638698   12.477342    -3.4839075\n",
      "    3.8686767    3.3661432   29.292826    23.380342    -9.822691\n",
      "    6.5085225    1.2674712    0.6454226    8.081476    17.698809\n",
      "   23.896338    -8.951108     1.8607738   17.704803    -4.401247\n",
      "   15.820031     6.7447367    4.06339     14.761637     5.8723235\n",
      "   28.894663    11.230736    17.081568     4.115218    15.530333\n",
      "  -18.336649    -0.11356454 -26.42633      6.2912626   -4.615173\n",
      "    9.431319    10.362668    20.173195     8.118363    -1.1363921\n",
      "   -0.02310816   3.3087537   20.046549    15.620966     3.5909011\n",
      "    7.432886    13.900438   -34.736546    -0.53630507  12.474585\n",
      "   10.027146    18.293526    13.149651   -20.229122    15.298475\n",
      "   10.008512    29.38841     14.421669     3.969407    -8.819838\n",
      "   -5.744976    26.096478    29.247688    -9.773905    12.192394\n",
      "    0.59972394 -11.52563     -9.195021    -9.549802     6.3527584\n",
      "   -1.2299515    4.5182295    2.7714682    1.8087555   12.130819\n",
      "    6.4694123   -3.0750017   10.130462   -20.545395    17.4074\n",
      "    2.3121536   11.417216     6.121464    16.061108    -5.439716\n",
      "    7.1571336   33.842175    20.556839    -2.6037323   19.71648\n",
      "   14.557402    -6.165629    33.28185     20.208021    13.190612\n",
      "    1.70461     27.647581    -4.751094    11.913344    14.04335\n",
      "   -2.9008882   17.309792    11.771074    32.433918    -6.602666\n",
      "   19.516891    -6.9954815   -0.8725539   -3.3375301   16.656954\n",
      "   11.103571   -17.346672     9.4245      -3.7803643  -10.207179\n",
      "  -16.100025     6.7656465   -1.7143266   -6.576982    -2.3413506\n",
      "    9.301916     7.8169165  -22.877497    28.293041    -9.703786\n",
      "  -16.757765     4.992949     9.544962    19.718817    -0.79521906\n",
      "   31.680733    -6.0621133   -7.7896595   -1.4615208  -23.399952\n",
      "   -5.311914    25.449995    22.404865    -7.6770144   -0.8586649\n",
      "    1.0967149    9.165818     9.193124     7.518168    10.298886\n",
      "   -4.922513    22.398523    31.428265    18.752428    20.433823\n",
      "   -7.4194574    0.6988133    1.5385753   -3.385963    11.042375\n",
      "   20.83201      0.87466466  25.956907    -3.1604345    6.7177095\n",
      "   10.669141    -3.9906914   -3.6018531    3.0734365   51.841915\n",
      "   -4.753723    26.79191     24.013327     4.7451315   -1.0970883\n",
      "    4.702057    -2.8807976   23.281279     6.6233435    8.097082\n",
      "  -13.089649     2.4368298    3.0566242  -24.799189   -11.215246\n",
      "    6.544868   -20.23283     17.099981    18.850136    22.841608\n",
      "   23.728941    -0.24853013  29.227602     6.8196507   17.516787\n",
      "    9.85838     15.353367    23.642902     2.83162     30.198414\n",
      "    9.454276    21.031227    10.741055    11.303082    13.231943\n",
      "    1.6242737   -1.1554645   12.008866    11.393256    -9.392363\n",
      "    3.8162894    1.7229798    7.742445    13.968282    20.39744\n",
      "   15.937774   -21.085085     2.4777536   32.81541     18.250422\n",
      "   11.402193   -24.098837    16.257364    -2.014268     5.132413\n",
      "   -7.8547134   -4.104288     2.6427162    4.0935535   -3.4419649\n",
      "    2.2782118   16.939411     0.5013574   15.49225    -16.249138\n",
      "   -3.0819364   17.250362    21.130802    27.692413    -1.2612098\n",
      "   18.409172    11.36895      5.5802565    4.2409616   11.872737\n",
      "   -3.5811477    9.312237    32.150597     2.3944492    4.443193\n",
      "   14.685976    23.383236     6.4837627   39.9934       6.70153\n",
      "   23.449625    21.384163    -6.979006     4.794963     7.2730446\n",
      "   10.548369    19.557909   -18.958157     8.760035     4.5856247\n",
      "   -0.8955       0.51528996  -2.3384712   25.007322    -9.998209\n",
      "    4.2805767   18.109453    24.513332    16.409393     0.37661362\n",
      "   12.420322    21.094757    16.5202       4.0781174   -3.05877\n",
      "   -5.1252756   33.143875    18.824621    12.637094     8.314735\n",
      "   24.23377     -7.7268467    0.4544178   13.198473     8.640726\n",
      "   -7.890362    19.577225    15.814135     8.666599     6.625137\n",
      "   18.385607    18.966726    65.30654      7.5088816    5.6766815\n",
      "    1.942221    -4.7899723   -7.053807     8.863577     0.36404273\n",
      "   19.101105    -5.881313   -12.722914    17.424103    22.993164\n",
      "    4.9563127    1.7909614   -6.589553    12.011394    17.637777\n",
      "   21.859184    13.378748   -11.285295     1.2376474   11.8076\n",
      "   -3.6467028   28.415483    -9.329626     3.5610332   13.6862545\n",
      "    5.470395    -4.1894674   -3.7024777    9.294284    -4.720895\n",
      "   -2.2741516    3.2630787  -14.0752735   21.65734     -2.413613\n",
      "   29.361006    24.156248   -13.683957    10.7251625   19.393232\n",
      "    0.6090313   -7.2221875    4.3369794   11.526634     1.2070285\n",
      "   18.207253     6.9439507    7.8813887    4.8823786   33.00792\n",
      "   13.855344     9.488827     1.5648425  -17.766068    10.88251\n",
      "   13.265294     7.154706     5.0548935  -14.159311     8.462216\n",
      "   24.918617     1.23286      6.086013    -5.0876327   10.308186\n",
      "   15.204173    10.174111    17.201738    13.270758    11.660455\n",
      "   16.797186    -7.459043     0.55158865   8.546268     1.170508\n",
      "   29.967186    21.68521     23.715643    -3.3831873   25.590538\n",
      "   18.48772     12.2867985   11.200556    16.859434    -5.2601175\n",
      "  -18.291403    -5.207982     6.4389186   20.304077    20.21515\n",
      "   -5.306515    29.309505    21.230461    12.440586     4.912999\n",
      "   30.53441      6.2858925   -7.5614915    6.5794578    7.0083327\n",
      "    2.287938    19.496271    37.777653   -17.589617     0.8128855\n",
      "   20.773415    14.031943    -0.90890414   5.498264     9.335044\n",
      "    4.687465   -15.74937     16.53449     25.261948    -3.270122\n",
      "   32.79216     36.771523    15.524856    17.17329    -36.491688\n",
      "   17.052538    17.831846    22.184786    19.646654    23.505194\n",
      "    5.9968224   22.784414    -4.0734973   17.62358      9.39946\n",
      "   21.73117      6.185536   -17.932117    39.83358     11.360685\n",
      "    7.2727113   20.369604     9.22288     16.668356    14.204637\n",
      "   23.126936     1.7179711    3.129214     5.776697    14.8726015\n",
      "   -9.659104    20.996578     1.3784677    0.178379    22.34389\n",
      "    7.154952    34.755142    -3.4229815   16.718513   -29.174107\n",
      "   -1.4050107    4.819188     1.620605    26.256773     4.960097\n",
      "   -7.4154267   14.536121   -11.573291    -6.6726627   -7.6787667\n",
      "  -34.87479     -3.2184377   13.316946    13.302086    -1.5566567\n",
      "  -11.522836    20.932041    11.668435    12.483268    25.515667\n",
      "   13.531062    20.079878     5.960796     9.691567     7.0971794\n",
      "    4.7716775   29.069233    32.69974     11.4374275   10.045294\n",
      "    2.5476043   -1.6946335    8.372069    26.996967   -13.56212\n",
      "   -3.139305     2.2818663    5.99774     13.805684   -16.287006\n",
      "    0.8192569   19.41478     16.050234     8.989544     6.044651\n",
      "   -3.7054882    9.351668    47.04641     11.058642    -1.838801\n",
      "    9.552935    20.75933     20.427427    -5.320199    11.551934\n",
      "    8.878507    13.225651    29.907566     7.6125274    6.763479\n",
      "    8.859828    11.576555    25.568977    19.14669     15.679273\n",
      "    5.1579156   19.384102    13.54092     11.714598    -4.9401617\n",
      "   -1.8365896    5.3065305   21.952349     0.47562903   0.38709483\n",
      "   -9.683731     8.743272     7.330867    23.95437     34.401367\n",
      "   19.376904     4.9440355   23.379911    20.468578     4.0907903\n",
      "   21.158443   -15.685961    -9.6276      -2.7819638   -4.8855476\n",
      "   -6.9708743    9.871918     4.402635    11.028353    38.442806\n",
      "   14.918587   -10.8141365   31.940933     7.4689207    2.7614436\n",
      "   17.057726    27.594444     3.9989927    1.8927134  -10.035443\n",
      "    7.5321317    0.4483087   12.998331    -9.8465395   -0.3831432\n",
      "    3.0528047   13.900195    26.267899     7.344693   -11.390349\n",
      "   15.936986    17.324669     4.066179    -9.369962    16.062151\n",
      "   -7.0507026    0.43684152   8.164668     2.7624395   16.995714\n",
      "    0.9882274    2.7714458    3.355825    10.1630335   24.692192\n",
      "   10.687821     2.9533331    2.021881    11.033693    21.564207\n",
      "   14.459721    11.113318    13.348296    10.178401     1.2674167\n",
      "    0.8698525    9.809979    18.80543    -13.272145    35.120457\n",
      "   19.664413    13.754169     5.5900936  -14.346786    16.834917\n",
      "   11.257751    11.983093     6.047374    20.046774    11.816628\n",
      "   -9.030362    32.021965    -8.591352    31.266684    13.666424\n",
      "   -1.4854826   17.741234     3.2289107   14.11073      9.344919\n",
      "    8.70253     -1.1768829  -14.616942    17.195889    33.631733\n",
      "    3.3066285   21.278923    -5.7278805    8.832183     6.304963\n",
      "   11.583508    -3.5857248  -27.603495    -6.6624193    9.694564\n",
      "    6.269835     7.690572    20.122976    -0.8189142   22.012197\n",
      "   -5.9225197   -2.5101159   42.356537     9.97608     11.520121\n",
      "   -5.1960173  -10.080663     5.252725    -9.656503     3.6378107\n",
      "    0.49082226  13.221282   -26.067465   -23.279442    34.391407\n",
      "   -4.834665   -12.082111    30.611961     8.592669     1.1328789\n",
      "    3.217358    21.096851    18.614805    -9.01824     -3.0341623\n",
      "  -10.02064     18.063011    -3.3791661   25.369858  ]]\n"
     ]
    }
   ],
   "source": [
    "testmodel = ChessModel(768)\n",
    "testmodel.load_state_dict(torch.load('/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/notebooks/runs/20240310_220627_eval_basic_4096/models/0.pth'))\n",
    "\n",
    "input = np.array([\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
    "1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "0, 0, 0, 0,\n",
    "])\n",
    "\n",
    "fl = testmodel.linear1\n",
    "r = fl(torch.tensor(input, dtype=torch.float32).reshape(-1, 768))\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize, suppress=True)\n",
    "print(np.round(fl.bias.detach().numpy()*127*64))\n",
    "print(r.detach().numpy()*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
