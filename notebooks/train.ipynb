{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lib.service import SamplesService\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_int64_bitset(x):\n",
    "    \"\"\"\n",
    "    Convert a 64-bit integer into a 64-element float tensor\n",
    "    \"\"\"\n",
    "    masks = 2 ** torch.arange(64, dtype=torch.int64, device='cuda')\n",
    "    expanded = torch.bitwise_and(x.unsqueeze(-1), masks).ne(0).to(torch.float32)\n",
    "    return expanded\n",
    "\n",
    "intermediate_scale = 1 / 64\n",
    "output_scale = 1 / 9600\n",
    "\n",
    "class ChessModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(ChessModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(num_features, 1024)\n",
    "        self.linear2 = torch.nn.Linear(1024, 64)\n",
    "        self.linear3 = torch.nn.Linear(64, 64)\n",
    "        self.output = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0) # Clipped ReLU\n",
    "        x = self.linear2(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0) # Clipped ReLU\n",
    "        x = self.linear3(x)\n",
    "        x = torch.clamp(x, 0.0, 1.0) # Clipped ReLU\n",
    "        return self.output(x)\n",
    "    \n",
    "    def _clip_weights(self):\n",
    "        intermediate_clip = 127 * intermediate_scale\n",
    "        output_clip = 127*127 * output_scale\n",
    "\n",
    "        self.linear1.weight.data.clamp_(-intermediate_clip, intermediate_clip)\n",
    "        self.linear2.weight.data.clamp_(-intermediate_clip, intermediate_clip)\n",
    "        self.linear3.weight.data.clamp_(-intermediate_clip, intermediate_clip)\n",
    "        self.output.weight.data.clamp_(-output_clip, output_clip)\n",
    "\n",
    "    def _layer_json(self, layer, scale_value):\n",
    "        weights = layer.weight.data.cpu().numpy()\n",
    "        bias = layer.bias.data.cpu().numpy()\n",
    "\n",
    "        qweights = np.round(weights / scale_value).astype('int8')\n",
    "        qbias = np.round(bias).astype('int32')\n",
    "\n",
    "        return {\n",
    "            \"in_features\": layer.in_features,\n",
    "            \"out_features\": layer.out_features,\n",
    "            \"weights\": qweights.tolist(),\n",
    "            \"bias\": qbias.tolist()\n",
    "        }\n",
    "\n",
    "    def to_json(self):\n",
    "        \"\"\"\n",
    "        Returns the layers of the model as a list of dictionaries\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "\n",
    "        for layer in [self.linear1, self.linear2, self.linear3]:\n",
    "            layers.append(self._layer_json(layer, intermediate_scale))\n",
    "        layers.append(self._layer_json(layer, output_scale))\n",
    "\n",
    "        return layers\n",
    "\n",
    "#testmodel = ChessModel(768)\n",
    "#testmodel.load_state_dict(torch.load('/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/notebooks/runs/20240308_130550_eval_basic_4096/models/400.pth'))\n",
    "#testmodel.cuda()\n",
    "\n",
    "#X, y = samples_service.next_batch()\n",
    "#X = decode_int64_bitset(X)\n",
    "#X = X.reshape(-1, NUM_FEATURES)\n",
    "#print(testmodel(X) / 356 * 100, y)\n",
    "\n",
    "#import json\n",
    "#with open('model.json', 'w') as f:\n",
    "#    f.write(json.dumps(testmodel.to_json()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PQRLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PQRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred):\n",
    "        pred = pred.reshape(-1, 3)\n",
    "        \n",
    "        p = pred[:,0]\n",
    "        q = pred[:,1]\n",
    "        r = pred[:,2]\n",
    "        \n",
    "        a = -torch.mean(torch.log(torch.sigmoid(r - q)))\n",
    "        b = torch.mean(torch.square(p + q))\n",
    "\n",
    "        loss = a + b\n",
    "\n",
    "        return loss\n",
    "\n",
    "class EvalLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, model, target):\n",
    "        scaling = 356.0\n",
    "\n",
    "        # scale CP score to engine units [-10_000, 10_000]\n",
    "        target = target * scaling / 100.0\n",
    "\n",
    "        # targets are in CP-space change it to WDL-space [0, 1]\n",
    "        wdl_model = torch.sigmoid(model / scaling)\n",
    "        wdl_target = torch.sigmoid(target / scaling)\n",
    "\n",
    "        loss = torch.pow(torch.abs(wdl_model - wdl_target), 2.5)\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 18:49:16.672023: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-08 18:49:16.672084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-08 18:49:16.673712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-08 18:49:16.685488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 18:49:17.994044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Epoch 0: 100%|██████████| 1000/1000 [00:09<00:00, 104.72it/s]\n",
      "Epoch 1: 100%|██████████| 1000/1000 [00:08<00:00, 112.64it/s]\n",
      "Epoch 2: 100%|██████████| 1000/1000 [00:10<00:00, 91.11it/s]\n",
      "Epoch 3: 100%|██████████| 1000/1000 [00:11<00:00, 90.16it/s]\n",
      "Epoch 4: 100%|██████████| 1000/1000 [00:12<00:00, 79.71it/s]\n",
      "Epoch 5: 100%|██████████| 1000/1000 [00:12<00:00, 78.45it/s]\n",
      "Epoch 6: 100%|██████████| 1000/1000 [00:11<00:00, 86.61it/s]\n",
      "Epoch 7: 100%|██████████| 1000/1000 [00:11<00:00, 86.44it/s]\n",
      "Epoch 8: 100%|██████████| 1000/1000 [00:14<00:00, 69.40it/s]\n",
      "Epoch 9: 100%|██████████| 1000/1000 [00:12<00:00, 82.64it/s]\n",
      "Epoch 10: 100%|██████████| 1000/1000 [00:09<00:00, 107.04it/s]\n",
      "Epoch 11: 100%|██████████| 1000/1000 [00:10<00:00, 95.29it/s]\n",
      "Epoch 12: 100%|██████████| 1000/1000 [00:11<00:00, 88.71it/s]\n",
      "Epoch 13: 100%|██████████| 1000/1000 [00:10<00:00, 94.49it/s]\n",
      "Epoch 14: 100%|██████████| 1000/1000 [00:09<00:00, 103.30it/s]\n",
      "Epoch 15: 100%|██████████| 1000/1000 [00:10<00:00, 94.43it/s]\n",
      "Epoch 16: 100%|██████████| 1000/1000 [00:09<00:00, 103.74it/s]\n",
      "Epoch 17: 100%|██████████| 1000/1000 [00:09<00:00, 104.80it/s]\n",
      "Epoch 18: 100%|██████████| 1000/1000 [00:09<00:00, 100.71it/s]\n",
      "Epoch 19: 100%|██████████| 1000/1000 [00:10<00:00, 96.26it/s]\n",
      "Epoch 20: 100%|██████████| 1000/1000 [00:09<00:00, 101.86it/s]\n",
      "Epoch 21: 100%|██████████| 1000/1000 [00:10<00:00, 95.94it/s]\n",
      "Epoch 22: 100%|██████████| 1000/1000 [00:09<00:00, 102.64it/s]\n",
      "Epoch 23: 100%|██████████| 1000/1000 [00:09<00:00, 103.95it/s]\n",
      "Epoch 24: 100%|██████████| 1000/1000 [00:09<00:00, 106.32it/s]\n",
      "Epoch 25: 100%|██████████| 1000/1000 [00:11<00:00, 87.52it/s]\n",
      "Epoch 26: 100%|██████████| 1000/1000 [00:09<00:00, 101.14it/s]\n",
      "Epoch 27: 100%|██████████| 1000/1000 [00:09<00:00, 103.94it/s]\n",
      "Epoch 28: 100%|██████████| 1000/1000 [00:10<00:00, 97.09it/s]\n",
      "Epoch 29: 100%|██████████| 1000/1000 [00:11<00:00, 88.84it/s]\n",
      "Epoch 30: 100%|██████████| 1000/1000 [00:09<00:00, 104.70it/s]\n",
      "Epoch 31: 100%|██████████| 1000/1000 [00:10<00:00, 98.47it/s]\n",
      "Epoch 32: 100%|██████████| 1000/1000 [00:09<00:00, 105.30it/s]\n",
      "Epoch 33: 100%|██████████| 1000/1000 [00:09<00:00, 102.69it/s]\n",
      "Epoch 34: 100%|██████████| 1000/1000 [00:11<00:00, 90.61it/s]\n",
      "Epoch 35: 100%|██████████| 1000/1000 [00:10<00:00, 91.77it/s]\n",
      "Epoch 36: 100%|██████████| 1000/1000 [00:11<00:00, 86.76it/s]\n",
      "Epoch 37: 100%|██████████| 1000/1000 [00:08<00:00, 117.09it/s]\n",
      "Epoch 38: 100%|██████████| 1000/1000 [00:08<00:00, 114.25it/s]\n",
      "Epoch 39: 100%|██████████| 1000/1000 [00:08<00:00, 114.00it/s]\n",
      "Epoch 40: 100%|██████████| 1000/1000 [00:09<00:00, 105.41it/s]\n",
      "Epoch 41: 100%|██████████| 1000/1000 [00:09<00:00, 102.90it/s]\n",
      "Epoch 42: 100%|██████████| 1000/1000 [00:10<00:00, 92.78it/s]\n",
      "Epoch 43: 100%|██████████| 1000/1000 [00:10<00:00, 93.00it/s]\n",
      "Epoch 44: 100%|██████████| 1000/1000 [00:10<00:00, 98.73it/s]\n",
      "Epoch 45: 100%|██████████| 1000/1000 [00:10<00:00, 98.01it/s]\n",
      "Epoch 46: 100%|██████████| 1000/1000 [00:09<00:00, 101.46it/s]\n",
      "Epoch 47: 100%|██████████| 1000/1000 [00:09<00:00, 102.52it/s]\n",
      "Epoch 48: 100%|██████████| 1000/1000 [00:09<00:00, 100.92it/s]\n",
      "Epoch 49: 100%|██████████| 1000/1000 [00:13<00:00, 75.40it/s]\n",
      "Epoch 50: 100%|██████████| 1000/1000 [00:12<00:00, 79.12it/s]\n",
      "Epoch 51: 100%|██████████| 1000/1000 [00:09<00:00, 103.72it/s]\n",
      "Epoch 52: 100%|██████████| 1000/1000 [00:09<00:00, 108.46it/s]\n",
      "Epoch 53: 100%|██████████| 1000/1000 [00:09<00:00, 109.53it/s]\n",
      "Epoch 54: 100%|██████████| 1000/1000 [00:08<00:00, 115.26it/s]\n",
      "Epoch 55: 100%|██████████| 1000/1000 [00:08<00:00, 116.42it/s]\n",
      "Epoch 56: 100%|██████████| 1000/1000 [00:08<00:00, 113.23it/s]\n",
      "Epoch 57: 100%|██████████| 1000/1000 [00:08<00:00, 116.26it/s]\n",
      "Epoch 58: 100%|██████████| 1000/1000 [00:09<00:00, 108.82it/s]\n",
      "Epoch 59: 100%|██████████| 1000/1000 [00:08<00:00, 112.65it/s]\n",
      "Epoch 60: 100%|██████████| 1000/1000 [00:08<00:00, 112.12it/s]\n",
      "Epoch 61: 100%|██████████| 1000/1000 [00:09<00:00, 105.54it/s]\n",
      "Epoch 62: 100%|██████████| 1000/1000 [00:09<00:00, 101.24it/s]\n",
      "Epoch 63: 100%|██████████| 1000/1000 [00:08<00:00, 111.54it/s]\n",
      "Epoch 64: 100%|██████████| 1000/1000 [00:09<00:00, 103.02it/s]\n",
      "Epoch 65: 100%|██████████| 1000/1000 [00:08<00:00, 112.58it/s]\n",
      "Epoch 66: 100%|██████████| 1000/1000 [00:10<00:00, 96.36it/s]\n",
      "Epoch 67: 100%|██████████| 1000/1000 [00:09<00:00, 101.49it/s]\n",
      "Epoch 68: 100%|██████████| 1000/1000 [00:10<00:00, 99.49it/s]\n",
      "Epoch 69: 100%|██████████| 1000/1000 [00:09<00:00, 104.08it/s]\n",
      "Epoch 70: 100%|██████████| 1000/1000 [00:08<00:00, 115.06it/s]\n",
      "Epoch 71: 100%|██████████| 1000/1000 [00:09<00:00, 107.00it/s]\n",
      "Epoch 72: 100%|██████████| 1000/1000 [00:12<00:00, 82.60it/s]\n",
      "Epoch 73: 100%|██████████| 1000/1000 [00:11<00:00, 88.13it/s]\n",
      "Epoch 74: 100%|██████████| 1000/1000 [00:11<00:00, 88.56it/s]\n",
      "Epoch 75: 100%|██████████| 1000/1000 [00:10<00:00, 98.99it/s] \n",
      "Epoch 76: 100%|██████████| 1000/1000 [00:09<00:00, 105.47it/s]\n",
      "Epoch 77: 100%|██████████| 1000/1000 [00:09<00:00, 105.98it/s]\n",
      "Epoch 78: 100%|██████████| 1000/1000 [00:10<00:00, 93.51it/s]\n",
      "Epoch 79: 100%|██████████| 1000/1000 [00:10<00:00, 97.82it/s]\n",
      "Epoch 80:  71%|███████   | 710/1000 [00:07<00:03, 96.40it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m X \u001b[38;5;241m=\u001b[39m decode_int64_bitset(X)\n\u001b[1;32m     71\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, NUM_FEATURES)\n\u001b[0;32m---> 73\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(avg_loss):\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(X, y):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# Clear the gradients\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m chessmodel(X)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[1;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/profiler.py:492\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecord \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "EPOCHS = 100000\n",
    "BATCHES_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "FEATURE_SET = \"basic\"\n",
    "NUM_FEATURES = 768\n",
    "METHOD = \"eval\"\n",
    "\n",
    "if METHOD == \"pqr\":\n",
    "    X_SHAPE = (BATCH_SIZE, 3, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 0)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/pqr-1700/*.csv\")\n",
    "    loss_fn = PQRLoss()\n",
    "elif METHOD == \"eval\":\n",
    "    X_SHAPE = (BATCH_SIZE, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 1)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/eval/*.csv\")\n",
    "    loss_fn = EvalLoss()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "folder = f'runs/{timestamp}_{METHOD}_{FEATURE_SET}_{BATCH_SIZE}'\n",
    "os.makedirs(f'{folder}/models', exist_ok=True)\n",
    "\n",
    "samples_service = SamplesService(x_shape=X_SHAPE, y_shape=Y_SHAPE, inputs=INPUTS, feature_set=FEATURE_SET, method=METHOD)\n",
    "chessmodel = ChessModel(num_features=NUM_FEATURES)\n",
    "chessmodel.cuda()\n",
    "\n",
    "#for i in tqdm(range(1000000)):\n",
    "#    a = samples_service.next_batch()\n",
    "\n",
    "optimizer = torch.optim.Adam(chessmodel.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.0001, factor=0.7, patience=10)\n",
    "writer = SummaryWriter(folder)\n",
    "\n",
    "# @torch.compile # 30% speedup\n",
    "def train_step(X, y):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = chessmodel(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    chessmodel._clip_weights()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Make sure gradient tracking is on\n",
    "chessmodel.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _ in tqdm(range(BATCHES_PER_EPOCH), desc=f'Epoch {epoch}'):\n",
    "        X, y = samples_service.next_batch()\n",
    "    \n",
    "        # expand bitset\n",
    "        X = decode_int64_bitset(X)\n",
    "        X = X.reshape(-1, NUM_FEATURES)\n",
    "\n",
    "        loss = train_step(X, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        if math.isnan(avg_loss):\n",
    "            raise Exception(\"Loss is NaN, exiting\")\n",
    "\n",
    "    avg_loss /= BATCHES_PER_EPOCH\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    writer.add_scalar('Train/loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train/lr', scheduler._last_lr[0], epoch) # get_last_lr()\n",
    "    writer.add_scalar('Params/mean-l1', torch.mean(chessmodel.linear1.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l2', torch.mean(chessmodel.linear2.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l3', torch.mean(chessmodel.linear3.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l4', torch.mean(chessmodel.output.weight), epoch)\n",
    "    for name, param in chessmodel.named_parameters():\n",
    "        writer.add_histogram(name, param, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # save model\n",
    "    torch.save(chessmodel.state_dict(), f'{folder}/models/{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(chessmodel.output.parameters())[0].cpu().detach().numpy()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# plot distribution\n",
    "sns.histplot(a.flatten(), kde=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
