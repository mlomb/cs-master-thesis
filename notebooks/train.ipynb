{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import tempfile\n",
    "\n",
    "from lib.service import SamplesService\n",
    "from lib.model import NnueModel\n",
    "from lib.model import decode_int64_bitset\n",
    "from lib.serialize import NnueWriter\n",
    "from lib.puzzles import PuzzleAccuracy\n",
    "from lib.losses import EvalLoss, PQRLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlomb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/notebooks/wandb/run-20240330_012659-i1ug8vr7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlomb/cs-master-thesis/runs/i1ug8vr7/workspace' target=\"_blank\">20240330_012657_eval_half-piece_256</a></strong> to <a href='https://wandb.ai/mlomb/cs-master-thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlomb/cs-master-thesis' target=\"_blank\">https://wandb.ai/mlomb/cs-master-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlomb/cs-master-thesis/runs/i1ug8vr7/workspace' target=\"_blank\">https://wandb.ai/mlomb/cs-master-thesis/runs/i1ug8vr7/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [00:05<00:00, 180.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.57it/s]\n",
      "Epoch 1: 100%|██████████| 1000/1000 [00:05<00:00, 180.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.08it/s]\n",
      "Epoch 2: 100%|██████████| 1000/1000 [00:05<00:00, 179.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.73it/s]\n",
      "Epoch 3: 100%|██████████| 1000/1000 [00:05<00:00, 192.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.05it/s]\n",
      "Epoch 4: 100%|██████████| 1000/1000 [00:04<00:00, 202.41it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.29it/s]\n",
      "Epoch 5: 100%|██████████| 1000/1000 [00:04<00:00, 201.77it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.77it/s]\n",
      "Epoch 6: 100%|██████████| 1000/1000 [00:05<00:00, 168.16it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.26it/s]\n",
      "Epoch 7: 100%|██████████| 1000/1000 [00:04<00:00, 236.42it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.75it/s]\n",
      "Epoch 8: 100%|██████████| 1000/1000 [00:04<00:00, 231.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.09it/s]\n",
      "Epoch 9: 100%|██████████| 1000/1000 [00:04<00:00, 215.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.05it/s]\n",
      "Epoch 10: 100%|██████████| 1000/1000 [00:04<00:00, 213.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.81it/s]\n",
      "Epoch 11: 100%|██████████| 1000/1000 [00:04<00:00, 214.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.21it/s]\n",
      "Epoch 12: 100%|██████████| 1000/1000 [00:04<00:00, 210.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.66it/s]\n",
      "Epoch 13: 100%|██████████| 1000/1000 [00:06<00:00, 156.07it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.61it/s]\n",
      "Epoch 14: 100%|██████████| 1000/1000 [00:05<00:00, 181.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.18it/s]\n",
      "Epoch 15: 100%|██████████| 1000/1000 [00:04<00:00, 203.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.12it/s]\n",
      "Epoch 16: 100%|██████████| 1000/1000 [00:04<00:00, 204.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.57it/s]\n",
      "Epoch 17: 100%|██████████| 1000/1000 [00:05<00:00, 197.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.46it/s]\n",
      "Epoch 18: 100%|██████████| 1000/1000 [00:05<00:00, 176.03it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.81it/s]\n",
      "Epoch 19: 100%|██████████| 1000/1000 [00:05<00:00, 175.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.83it/s]\n",
      "Epoch 20: 100%|██████████| 1000/1000 [00:06<00:00, 163.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.93it/s]\n",
      "Epoch 21: 100%|██████████| 1000/1000 [00:05<00:00, 183.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.66it/s]\n",
      "Epoch 22: 100%|██████████| 1000/1000 [00:04<00:00, 211.16it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.24it/s]\n",
      "Epoch 23: 100%|██████████| 1000/1000 [00:04<00:00, 233.02it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.13it/s]\n",
      "Epoch 24: 100%|██████████| 1000/1000 [00:05<00:00, 180.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.49it/s]\n",
      "Epoch 25: 100%|██████████| 1000/1000 [00:04<00:00, 214.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.38it/s]\n",
      "Epoch 26: 100%|██████████| 1000/1000 [00:05<00:00, 184.41it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.12it/s]\n",
      "Epoch 27: 100%|██████████| 1000/1000 [00:04<00:00, 225.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.73it/s]\n",
      "Epoch 28: 100%|██████████| 1000/1000 [00:04<00:00, 220.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.31it/s]\n",
      "Epoch 29: 100%|██████████| 1000/1000 [00:04<00:00, 214.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.35it/s]\n",
      "Epoch 30: 100%|██████████| 1000/1000 [00:04<00:00, 216.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.44it/s]\n",
      "Epoch 31: 100%|██████████| 1000/1000 [00:04<00:00, 225.22it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.49it/s]\n",
      "Epoch 32: 100%|██████████| 1000/1000 [00:04<00:00, 206.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.71it/s]\n",
      "Epoch 33: 100%|██████████| 1000/1000 [00:04<00:00, 214.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.68it/s]\n",
      "Epoch 34: 100%|██████████| 1000/1000 [00:04<00:00, 208.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.80it/s]\n",
      "Epoch 35: 100%|██████████| 1000/1000 [00:04<00:00, 220.93it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.62it/s]\n",
      "Epoch 36: 100%|██████████| 1000/1000 [00:04<00:00, 218.09it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.12it/s]\n",
      "Epoch 37: 100%|██████████| 1000/1000 [00:06<00:00, 143.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.10it/s]\n",
      "Epoch 38: 100%|██████████| 1000/1000 [00:04<00:00, 208.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.00it/s]\n",
      "Epoch 39: 100%|██████████| 1000/1000 [00:05<00:00, 168.97it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.78it/s]\n",
      "Epoch 40: 100%|██████████| 1000/1000 [00:04<00:00, 217.33it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.08it/s]\n",
      "Epoch 41: 100%|██████████| 1000/1000 [00:05<00:00, 174.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.03it/s]\n",
      "Epoch 42: 100%|██████████| 1000/1000 [00:05<00:00, 198.45it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.28it/s]\n",
      "Epoch 43: 100%|██████████| 1000/1000 [00:05<00:00, 192.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.06it/s]\n",
      "Epoch 44: 100%|██████████| 1000/1000 [00:05<00:00, 190.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.13it/s]\n",
      "Epoch 45: 100%|██████████| 1000/1000 [00:06<00:00, 161.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.23it/s]\n",
      "Epoch 46: 100%|██████████| 1000/1000 [00:04<00:00, 205.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.27it/s]\n",
      "Epoch 47: 100%|██████████| 1000/1000 [00:05<00:00, 199.82it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.68it/s]\n",
      "Epoch 48: 100%|██████████| 1000/1000 [00:05<00:00, 197.41it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.99it/s]\n",
      "Epoch 49: 100%|██████████| 1000/1000 [00:04<00:00, 200.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.86it/s]\n",
      "Epoch 50: 100%|██████████| 1000/1000 [00:04<00:00, 206.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.72it/s]\n",
      "Epoch 51: 100%|██████████| 1000/1000 [00:04<00:00, 210.91it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.56it/s]\n",
      "Epoch 52: 100%|██████████| 1000/1000 [00:05<00:00, 199.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.22it/s]\n",
      "Epoch 53: 100%|██████████| 1000/1000 [00:05<00:00, 190.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.75it/s]\n",
      "Epoch 54: 100%|██████████| 1000/1000 [00:05<00:00, 195.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.85it/s]\n",
      "Epoch 55: 100%|██████████| 1000/1000 [00:05<00:00, 187.45it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.28it/s]\n",
      "Epoch 56: 100%|██████████| 1000/1000 [00:04<00:00, 214.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.43it/s]\n",
      "Epoch 57: 100%|██████████| 1000/1000 [00:05<00:00, 179.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.82it/s]\n",
      "Epoch 58: 100%|██████████| 1000/1000 [00:05<00:00, 195.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.07it/s]\n",
      "Epoch 59: 100%|██████████| 1000/1000 [00:04<00:00, 202.63it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.12it/s]\n",
      "Epoch 60: 100%|██████████| 1000/1000 [00:05<00:00, 191.63it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.33it/s]\n",
      "Epoch 61: 100%|██████████| 1000/1000 [00:04<00:00, 205.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.67it/s]\n",
      "Epoch 62: 100%|██████████| 1000/1000 [00:05<00:00, 169.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.74it/s]\n",
      "Epoch 63: 100%|██████████| 1000/1000 [00:05<00:00, 188.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.76it/s]\n",
      "Epoch 64: 100%|██████████| 1000/1000 [00:04<00:00, 201.41it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.41it/s]\n",
      "Epoch 65: 100%|██████████| 1000/1000 [00:04<00:00, 236.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.91it/s]\n",
      "Epoch 66: 100%|██████████| 1000/1000 [00:04<00:00, 238.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.46it/s]\n",
      "Epoch 67: 100%|██████████| 1000/1000 [00:04<00:00, 207.55it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.80it/s]\n",
      "Epoch 68: 100%|██████████| 1000/1000 [00:04<00:00, 204.39it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.45it/s]\n",
      "Epoch 69: 100%|██████████| 1000/1000 [00:04<00:00, 207.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.49it/s]\n",
      "Epoch 70: 100%|██████████| 1000/1000 [00:04<00:00, 208.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.11it/s]\n",
      "Epoch 71: 100%|██████████| 1000/1000 [00:04<00:00, 212.02it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.81it/s]\n",
      "Epoch 72: 100%|██████████| 1000/1000 [00:04<00:00, 221.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.68it/s]\n",
      "Epoch 73: 100%|██████████| 1000/1000 [00:04<00:00, 217.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.99it/s]\n",
      "Epoch 74: 100%|██████████| 1000/1000 [00:05<00:00, 191.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.57it/s]\n",
      "Epoch 75: 100%|██████████| 1000/1000 [00:04<00:00, 209.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.78it/s]\n",
      "Epoch 76: 100%|██████████| 1000/1000 [00:04<00:00, 206.97it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.18it/s]\n",
      "Epoch 77: 100%|██████████| 1000/1000 [00:04<00:00, 208.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.81it/s]\n",
      "Epoch 78: 100%|██████████| 1000/1000 [00:04<00:00, 235.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.73it/s]\n",
      "Epoch 79: 100%|██████████| 1000/1000 [00:04<00:00, 233.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.69it/s]\n",
      "Epoch 80: 100%|██████████| 1000/1000 [00:04<00:00, 239.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.36it/s]\n",
      "Epoch 81: 100%|██████████| 1000/1000 [00:04<00:00, 246.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.93it/s]\n",
      "Epoch 82: 100%|██████████| 1000/1000 [00:04<00:00, 237.68it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.94it/s]\n",
      "Epoch 83: 100%|██████████| 1000/1000 [00:05<00:00, 190.48it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.44it/s]\n",
      "Epoch 84: 100%|██████████| 1000/1000 [00:04<00:00, 204.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.82it/s]\n",
      "Epoch 85: 100%|██████████| 1000/1000 [00:05<00:00, 181.02it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.04it/s]\n",
      "Epoch 86: 100%|██████████| 1000/1000 [00:05<00:00, 179.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.92it/s]\n",
      "Epoch 87: 100%|██████████| 1000/1000 [00:05<00:00, 195.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.27it/s]\n",
      "Epoch 88: 100%|██████████| 1000/1000 [00:04<00:00, 204.05it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.55it/s]\n",
      "Epoch 89: 100%|██████████| 1000/1000 [00:04<00:00, 228.48it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.65it/s]\n",
      "Epoch 90: 100%|██████████| 1000/1000 [00:04<00:00, 227.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.12it/s]\n",
      "Epoch 91: 100%|██████████| 1000/1000 [00:05<00:00, 188.77it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.69it/s]\n",
      "Epoch 92: 100%|██████████| 1000/1000 [00:04<00:00, 200.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.08it/s]\n",
      "Epoch 93: 100%|██████████| 1000/1000 [00:04<00:00, 210.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.62it/s]\n",
      "Epoch 94: 100%|██████████| 1000/1000 [00:05<00:00, 183.87it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.58it/s]\n",
      "Epoch 95: 100%|██████████| 1000/1000 [00:04<00:00, 203.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.05it/s]\n",
      "Epoch 96: 100%|██████████| 1000/1000 [00:04<00:00, 236.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.05it/s]\n",
      "Epoch 97: 100%|██████████| 1000/1000 [00:04<00:00, 247.01it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.86it/s]\n",
      "Epoch 98: 100%|██████████| 1000/1000 [00:04<00:00, 210.23it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.36it/s]\n",
      "Epoch 99: 100%|██████████| 1000/1000 [00:05<00:00, 188.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 17.65it/s]\n",
      "Epoch 100: 100%|██████████| 1000/1000 [00:05<00:00, 172.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.21it/s]\n",
      "Epoch 101: 100%|██████████| 1000/1000 [00:05<00:00, 169.52it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.36it/s]\n",
      "Epoch 102: 100%|██████████| 1000/1000 [00:05<00:00, 195.21it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.81it/s]\n",
      "Epoch 103: 100%|██████████| 1000/1000 [00:04<00:00, 225.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.03it/s]\n",
      "Epoch 104: 100%|██████████| 1000/1000 [00:04<00:00, 202.21it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.27it/s]\n",
      "Epoch 105: 100%|██████████| 1000/1000 [00:05<00:00, 191.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.04it/s]\n",
      "Epoch 106: 100%|██████████| 1000/1000 [00:04<00:00, 233.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.68it/s]\n",
      "Epoch 107: 100%|██████████| 1000/1000 [00:04<00:00, 218.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.94it/s]\n",
      "Epoch 108: 100%|██████████| 1000/1000 [00:04<00:00, 234.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.38it/s]\n",
      "Epoch 109: 100%|██████████| 1000/1000 [00:04<00:00, 232.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.23it/s]\n",
      "Epoch 110: 100%|██████████| 1000/1000 [00:04<00:00, 217.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.31it/s]\n",
      "Epoch 111: 100%|██████████| 1000/1000 [00:05<00:00, 191.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 18.36it/s]\n",
      "Epoch 112: 100%|██████████| 1000/1000 [00:06<00:00, 161.41it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.09it/s]\n",
      "Epoch 113: 100%|██████████| 1000/1000 [00:04<00:00, 214.35it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.16it/s]\n",
      "Epoch 114: 100%|██████████| 1000/1000 [00:05<00:00, 167.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 19.94it/s]\n",
      "Epoch 115:  90%|████████▉ | 899/1000 [00:04<00:00, 184.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m X \u001b[38;5;241m=\u001b[39m decode_int64_bitset(X)\n\u001b[1;32m     75\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, NUM_FEATURES)\n\u001b[0;32m---> 77\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39misnan(avg_loss):\n",
      "Cell \u001b[0;32mIn[2], line 55\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Update the parameters\u001b[39;00m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 100000\n",
    "BATCHES_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 256 # 4096\n",
    "\n",
    "FEATURE_SET = \"half-piece\"\n",
    "NUM_FEATURES = 768 # 192 768 40960\n",
    "METHOD = \"eval\"\n",
    "\n",
    "if METHOD == \"pqr\":\n",
    "    X_SHAPE = (BATCH_SIZE, 3, 2, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 0)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/pqr-1700/*.csv\")\n",
    "    loss_fn = PQRLoss()\n",
    "elif METHOD == \"eval\":\n",
    "    X_SHAPE = (BATCH_SIZE, 2, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 1)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/eval/*.csv\")\n",
    "    loss_fn = EvalLoss()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f'{timestamp}_{METHOD}_{FEATURE_SET}_{BATCH_SIZE}'\n",
    "run = wandb.init(\n",
    "    project=\"cs-master-thesis\",\n",
    "    name=run_name,\n",
    "    job_type=\"train\",\n",
    "    config={\n",
    "        \"feature_set\": FEATURE_SET,\n",
    "        \"method\": METHOD,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"batches_per_epoch\": BATCHES_PER_EPOCH,\n",
    "    }\n",
    ")\n",
    "\n",
    "puzzles = PuzzleAccuracy('/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/puzzles.csv')\n",
    "samples_service = SamplesService(x_shape=X_SHAPE, y_shape=Y_SHAPE, inputs=INPUTS, feature_set=FEATURE_SET, method=METHOD)\n",
    "chessmodel = NnueModel(num_features=NUM_FEATURES)\n",
    "chessmodel.cuda()\n",
    "\n",
    "#for i in tqdm(range(1000000)):\n",
    "#    a = samples_service.next_batch()\n",
    "\n",
    "optimizer = torch.optim.Adam(chessmodel.parameters(), lr=0.0015)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.00001, factor=0.7, patience=100)\n",
    "\n",
    "# @torch.compile # 30% speedup\n",
    "def train_step(X, y):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = chessmodel(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    chessmodel.clip_weights()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Make sure gradient tracking is on\n",
    "chessmodel.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _ in tqdm(range(BATCHES_PER_EPOCH), desc=f'Epoch {epoch}'):\n",
    "        X, y = samples_service.next_batch()\n",
    "    \n",
    "        # expand bitset\n",
    "        X = decode_int64_bitset(X)\n",
    "        X = X.reshape(-1, 2, NUM_FEATURES)\n",
    "\n",
    "        loss = train_step(X, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        if math.isnan(avg_loss):\n",
    "            raise Exception(\"Loss is NaN, exiting\")\n",
    "\n",
    "    avg_loss /= BATCHES_PER_EPOCH\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # save model\n",
    "    with tempfile.NamedTemporaryFile() as tmp:\n",
    "        tmp.write(NnueWriter(chessmodel, FEATURE_SET).buf)\n",
    "    \n",
    "        # store artifact in W&B\n",
    "        artifact = wandb.Artifact(run_name, type=\"model\")\n",
    "        artifact.add_file(tmp.name, name=f\"{epoch}.nn\")\n",
    "        wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "\n",
    "        # run puzzles\n",
    "        puzzles_ratings, puzzles_accuracy = puzzles.measure([\"/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/engine/target/release/engine\", f\"--nn={tmp.name}\"])\n",
    "\n",
    "    # log metrics to W&B\n",
    "    wandb.log(step=epoch, data={\n",
    "        \"Train/loss\": avg_loss,\n",
    "        \"Train/lr\": scheduler._last_lr[0], # get_last_lr()\n",
    "\n",
    "        \"Weight/mean-f1\": torch.mean(chessmodel.ft.weight),\n",
    "        \"Weight/mean-l1\": torch.mean(chessmodel.linear1.weight),\n",
    "        \"Weight/mean-l2\": torch.mean(chessmodel.linear2.weight),\n",
    "        \"Weight/mean-out\": torch.mean(chessmodel.output.weight),\n",
    "\n",
    "        \"Puzzles/accuracy\": puzzles_accuracy\n",
    "    })\n",
    "    for rating_min, rating_max, accuracy in puzzles_ratings:\n",
    "        wandb.log(step=epoch, data={\n",
    "            f\"PuzzlesRating/{rating_min}-{rating_max}-accuracy\": accuracy\n",
    "        })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
