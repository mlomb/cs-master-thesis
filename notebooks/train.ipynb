{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print([dev.name for dev in device_lib.list_local_devices()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from glob import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"../data/dataset/2018_1600.bin\")\n",
    "record_size = 3 * 12 * 8\n",
    "batch_size = 4096\n",
    "dataset_size = sum([os.path.getsize(f) for f in files]) / record_size\n",
    "batches_per_epoch = math.ceil(dataset_size / batch_size)\n",
    "\n",
    "dataset = tf.data.FixedLengthRecordDataset(filenames=files, record_bytes=record_size)\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.apply(tf.data.experimental.copy_to_device('/gpu:0'))\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "#tfds.benchmark(dataset, batch_size=batch_size)\n",
    "\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CallbackList\n",
    "from lib.encoding import encode_board, decode_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_pred):\n",
    "    \"\"\"\n",
    "    Compute loss as defined in https://erikbern.com/2014/11/29/deep-learning-for-chess.html\n",
    "    // sum(p,q,r)logS(f(q)−f(r))+K*log(f(p)+f(q))+K*log(−f(q)−f(p))\n",
    "    \"\"\"\n",
    "    p = y_pred[:,0]\n",
    "    q = y_pred[:,1]\n",
    "    r = y_pred[:,2]\n",
    "    K = 1.0\n",
    "\n",
    "    rq_diff = r - q\n",
    "    pq_diff = K * (p + q)\n",
    "\n",
    "    a = - tf.math.reduce_mean(tf.math.log(tf.math.sigmoid(rq_diff)))\n",
    "    b = - tf.math.reduce_mean(tf.math.log(tf.math.sigmoid( pq_diff)))\n",
    "    c = - tf.math.reduce_mean(tf.math.log(tf.math.sigmoid(-pq_diff)))\n",
    "\n",
    "    reg = 0.0 # L2\n",
    "    for x in chess_model.trainable_variables:\n",
    "        reg += 0.01 * tf.math.reduce_mean(tf.math.square(x))\n",
    "\n",
    "    loss = a + b + c\n",
    "    obj = loss + reg\n",
    "\n",
    "    return loss, obj\n",
    "\n",
    "def make_chess_model():\n",
    "    inp = tf.keras.Input(shape=(12,), dtype=tf.int64)\n",
    "    x = decode_board(inp) # convert 12 ints to 768 floats\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(1)(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "chess_model = make_chess_model()\n",
    "chess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = int(time())\n",
    "epochs = 99999\n",
    "#optimizer = SGD(learning_rate=0.03, nesterov=True, momentum=0.9, clipnorm=1)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=100 * 500,\n",
    "    decay_rate=0.98,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "obj_tracker = keras.metrics.Mean(name=\"obj\")\n",
    "callbacks = CallbackList([\n",
    "    ModelCheckpoint(f\"checkpoints/{ts}\" + \"/model-{epoch:04d}-{loss:.3f}.keras\", monitor=\"loss\", save_best_only=True),\n",
    "    TensorBoard(log_dir=f\"./logs/{ts}\", write_graph=False)    \n",
    "], model=chess_model)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    batch = tf.reshape(tf.io.decode_raw(batch, tf.int64), (-1, 3, 12))\n",
    "\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = tf.reshape(chess_model(tf.reshape(batch, (-1, 12)), training=True), (-1, 3))  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value, obj_value = custom_loss(logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(obj_value, chess_model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, chess_model.trainable_weights))\n",
    "\n",
    "    # Update metrics\n",
    "    loss_tracker.update_state(loss_value)\n",
    "    obj_tracker.update_state(obj_value)\n",
    "\n",
    "callbacks.on_train_begin()\n",
    "for epoch in range(epochs):\n",
    "    loss_tracker.reset_states()\n",
    "    obj_tracker.reset_states()\n",
    "    callbacks.on_epoch_begin(epoch)\n",
    "\n",
    "    batch_i = 0\n",
    "    batch_i_last = 0\n",
    "    with tqdm(total=batches_per_epoch, bar_format=f\"Epoch {epoch+1}/{epochs}\" + \" {l_bar}{bar:10}{r_bar}{bar:-10b}\") as pbar:\n",
    "        for batch in dataset:\n",
    "            train_step(batch)\n",
    "\n",
    "            batch_i += 1\n",
    "            if batch_i % 10 == 0 or batch_i == batches_per_epoch:\n",
    "                pbar.set_postfix_str(f\"loss={loss_tracker.result():.4f} obj={obj_tracker.result():.4f} lr={optimizer.learning_rate.numpy():.4f}\")\n",
    "                pbar.update(batch_i - batch_i_last)\n",
    "                batch_i_last = batch_i\n",
    "\n",
    "    mean_tw = sum([tf.reduce_mean(tf.abs(tw)).numpy() for tw in chess_model.trainable_weights])\n",
    "\n",
    "    import chess\n",
    "    good_board = chess.Board(\"2kr3r/1pp1pp1p/1p6/q4bP1/2B5/4BP2/Pb1NQK1P/R2R4 w - - 1 18\")\n",
    "    bad_board = chess.Board(\"2kr3r/1pp1pp1p/1p6/q4bP1/2B5/4BP2/Pb1NQK1P/R1R5 w - - 1 18\")\n",
    "    pred = chess_model.predict(tf.concat([encode_board(good_board), encode_board(bad_board)], axis=0), verbose=0)\n",
    "\n",
    "    callbacks.on_epoch_end(epoch, logs={\n",
    "        \"loss\": loss_tracker.result(),\n",
    "        \"obj\": obj_tracker.result(),\n",
    "        \"lr\": optimizer.learning_rate.numpy(),\n",
    "        \"good\": pred[0][0],\n",
    "        \"bad\": pred[1][0],\n",
    "        \"mean_tw\": mean_tw\n",
    "    })\n",
    "callbacks.on_train_end()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
