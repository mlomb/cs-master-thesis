{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print([dev.name for dev in device_lib.list_local_devices()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from glob import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39098455.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(\"../data/dataset/*_1600.bin\")\n",
    "record_size = 3 * 12 * 8\n",
    "batch_size = 4096\n",
    "dataset_size = sum([os.path.getsize(f) for f in files]) / record_size\n",
    "batches_per_epoch = math.ceil(dataset_size / batch_size)\n",
    "\n",
    "dataset = tf.data.FixedLengthRecordDataset(filenames=files, record_bytes=record_size)\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.apply(tf.data.experimental.copy_to_device('/gpu:0'))\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "#tfds.benchmark(dataset, batch_size=batch_size)\n",
    "\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CallbackList, ReduceLROnPlateau\n",
    "from lib.encoding import encode_board, decode_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12)]              0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda  (None, 12, 1)             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.bitwise.bitwise_and (TF  (None, 12, 64)            0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " tf.math.not_equal (TFOpLam  (None, 12, 64)            0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.cast (TFOpLambda)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328705 (1.25 MB)\n",
      "Trainable params: 328705 (1.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def custom_loss(y_pred):\n",
    "    \"\"\"\n",
    "    Compute loss as defined in https://erikbern.com/2014/11/29/deep-learning-for-chess.html\n",
    "    // sum(p,q,r)logS(f(q)−f(r))+K*log(f(p)+f(q))+K*log(−f(q)−f(p))\n",
    "    \"\"\"\n",
    "    p = y_pred[:,0]\n",
    "    q = y_pred[:,1]\n",
    "    r = y_pred[:,2]\n",
    "\n",
    "    a = -tf.math.reduce_mean(tf.math.log(tf.math.sigmoid(r - q)))\n",
    "    b = tf.math.reduce_mean(tf.math.square(p + q))\n",
    "\n",
    "    reg = 0.0 # L2\n",
    "    for x in chess_model.trainable_variables:\n",
    "        reg += 0.01 * tf.math.reduce_mean(tf.math.square(x))\n",
    "\n",
    "    loss = a + b\n",
    "    obj = loss + reg\n",
    "\n",
    "    return loss, obj\n",
    "\n",
    "def make_chess_model():\n",
    "    inp = tf.keras.Input(shape=(12,), dtype=tf.int64)\n",
    "    x = decode_board(inp) # convert 12 ints to 768 floats\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(1)(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "chess_model = make_chess_model()\n",
    "chess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/99999   0%|          | 0/9546 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706661946.205199   22663 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1/99999 100%|██████████| 9546/9546 [01:51<00:00, 85.43it/s, loss=0.6105 obj=0.6114 lr=0.0100]\n",
      "Epoch 2/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.63it/s, loss=0.5497 obj=0.5530 lr=0.0100]\n",
      "Epoch 3/99999 100%|██████████| 9546/9546 [01:49<00:00, 87.53it/s, loss=0.5304 obj=0.5361 lr=0.0100]\n",
      "Epoch 4/99999 100%|██████████| 9546/9546 [01:47<00:00, 89.07it/s, loss=0.5230 obj=0.5301 lr=0.0100]\n",
      "Epoch 5/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.43it/s, loss=0.5188 obj=0.5269 lr=0.0100]\n",
      "Epoch 6/99999 100%|██████████| 9546/9546 [01:48<00:00, 88.06it/s, loss=0.5215 obj=0.5302 lr=0.0100]\n",
      "Epoch 7/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.62it/s, loss=0.5198 obj=0.5289 lr=0.0100]\n",
      "Epoch 8/99999 100%|██████████| 9546/9546 [01:48<00:00, 87.63it/s, loss=0.5153 obj=0.5245 lr=0.0100]\n",
      "Epoch 9/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.62it/s, loss=0.5136 obj=0.5232 lr=0.0100]\n",
      "Epoch 10/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.59it/s, loss=0.5124 obj=0.5222 lr=0.0100]\n",
      "Epoch 11/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.70it/s, loss=0.5118 obj=0.5217 lr=0.0100]\n",
      "Epoch 12/99999 100%|██████████| 9546/9546 [01:48<00:00, 87.77it/s, loss=0.5111 obj=0.5212 lr=0.0100]\n",
      "Epoch 13/99999 100%|██████████| 9546/9546 [01:48<00:00, 88.19it/s, loss=0.5107 obj=0.5209 lr=0.0100]\n",
      "Epoch 14/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.59it/s, loss=0.5104 obj=0.5206 lr=0.0100]\n",
      "Epoch 15/99999 100%|██████████| 9546/9546 [01:47<00:00, 88.73it/s, loss=0.5101 obj=0.5203 lr=0.0100]\n",
      "Epoch 16/99999 100%|██████████| 9546/9546 [01:47<00:00, 89.07it/s, loss=0.5100 obj=0.5202 lr=0.0100]\n",
      "Epoch 17/99999 100%|██████████| 9546/9546 [01:52<00:00, 85.22it/s, loss=0.5101 obj=0.5204 lr=0.0100]\n",
      "Epoch 18/99999 100%|██████████| 9546/9546 [01:55<00:00, 82.97it/s, loss=0.5095 obj=0.5198 lr=0.0100]\n",
      "Epoch 19/99999 100%|██████████| 9546/9546 [01:48<00:00, 87.79it/s, loss=0.5092 obj=0.5196 lr=0.0100]\n",
      "Epoch 20/99999 100%|██████████| 9546/9546 [01:45<00:00, 90.64it/s, loss=0.5092 obj=0.5197 lr=0.0100]\n",
      "Epoch 21/99999 100%|██████████| 9546/9546 [01:48<00:00, 88.15it/s, loss=0.5090 obj=0.5194 lr=0.0100]\n",
      "Epoch 22/99999 100%|██████████| 9546/9546 [01:51<00:00, 85.57it/s, loss=0.5089 obj=0.5193 lr=0.0100]\n",
      "Epoch 23/99999 100%|██████████| 9546/9546 [01:49<00:00, 86.90it/s, loss=0.5087 obj=0.5192 lr=0.0100] \n",
      "Epoch 24/99999 100%|██████████| 9546/9546 [02:04<00:00, 76.67it/s, loss=0.5086 obj=0.5191 lr=0.0100]\n",
      "Epoch 25/99999 100%|██████████| 9546/9546 [01:52<00:00, 85.19it/s, loss=0.5085 obj=0.5190 lr=0.0100]\n",
      "Epoch 26/99999 100%|██████████| 9546/9546 [01:51<00:00, 85.63it/s, loss=0.5085 obj=0.5190 lr=0.0100]\n",
      "Epoch 27/99999 100%|██████████| 9546/9546 [01:49<00:00, 87.03it/s, loss=0.5084 obj=0.5189 lr=0.0100] \n",
      "Epoch 28/99999 100%|██████████| 9546/9546 [01:39<00:00, 95.58it/s, loss=0.5083 obj=0.5188 lr=0.0100] \n",
      "Epoch 29/99999 100%|██████████| 9546/9546 [01:43<00:00, 92.29it/s, loss=0.5099 obj=0.5205 lr=0.0100] \n",
      "Epoch 30/99999 100%|██████████| 9546/9546 [01:42<00:00, 92.94it/s, loss=0.5084 obj=0.5189 lr=0.0100] \n",
      "Epoch 31/99999 100%|██████████| 9546/9546 [01:38<00:00, 96.74it/s, loss=0.5083 obj=0.5189 lr=0.0100] \n",
      "Epoch 32/99999 100%|██████████| 9546/9546 [01:38<00:00, 96.45it/s, loss=0.5083 obj=0.5188 lr=0.0100] \n",
      "Epoch 33/99999  69%|██████▊   | 6540/9546 [01:14<00:39, 75.55it/s, loss=0.5080 obj=0.5185 lr=0.0100] "
     ]
    }
   ],
   "source": [
    "name = f\"{int(time())}-256-rq+mse\"\n",
    "epochs = 99999\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "obj_tracker = keras.metrics.Mean(name=\"obj\")\n",
    "callbacks = CallbackList([\n",
    "    ModelCheckpoint(f\"checkpoints/{name}\" + \"/model-{epoch:04d}-{loss:.3f}.keras\", monitor=\"loss\", save_best_only=True),\n",
    "    TensorBoard(log_dir=f\"./logs/{name}\", write_graph=False)\n",
    "], model=chess_model)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    batch = tf.reshape(tf.io.decode_raw(batch, tf.int64), (-1, 3, 12))\n",
    "\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = tf.reshape(chess_model(tf.reshape(batch, (-1, 12)), training=True), (-1, 3))  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value, obj_value = custom_loss(logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(obj_value, chess_model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, chess_model.trainable_weights))\n",
    "\n",
    "    # Update metrics\n",
    "    loss_tracker.update_state(loss_value)\n",
    "    obj_tracker.update_state(obj_value)\n",
    "\n",
    "prev_loss = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "callbacks.on_train_begin()\n",
    "for epoch in range(epochs):\n",
    "    loss_tracker.reset_states()\n",
    "    obj_tracker.reset_states()\n",
    "    callbacks.on_epoch_begin(epoch)\n",
    "\n",
    "    if math.isnan(loss_tracker.result()):\n",
    "        print(\"Loss is NaN, exiting\")\n",
    "        break\n",
    "\n",
    "    batch_i = 0\n",
    "    batch_i_last = 0\n",
    "    with tqdm(total=batches_per_epoch, bar_format=f\"Epoch {epoch+1}/{epochs}\" + \" {l_bar}{bar:10}{r_bar}{bar:-10b}\") as pbar:\n",
    "        for batch in dataset:\n",
    "            train_step(batch)\n",
    "\n",
    "            batch_i += 1\n",
    "            if batch_i % 10 == 0 or batch_i == batches_per_epoch:\n",
    "                pbar.set_postfix_str(f\"loss={loss_tracker.result():.4f} obj={obj_tracker.result():.4f} lr={optimizer.learning_rate.numpy():.4f}\")\n",
    "                pbar.update(batch_i - batch_i_last)\n",
    "                batch_i_last = batch_i\n",
    "\n",
    "    mean_tw = sum([tf.reduce_mean(tf.abs(tw)).numpy() for tw in chess_model.trainable_weights])\n",
    "\n",
    "    epoch_loss = loss_tracker.result()\n",
    "\n",
    "    logs = {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"obj\": obj_tracker.result(),\n",
    "        \"lr\": optimizer.learning_rate.numpy(),\n",
    "        \"mean_tw\": mean_tw\n",
    "    }\n",
    "\n",
    "    # NOTE: GOOD/BAD based on the POV of WHO JUST PLAYED\n",
    "    samples = [\n",
    "        # 2kr3r/1pp1pp1p/1p6/q4bP1/2B5/4BP2/Pb1NQK1P/R6R w - - 0 18\n",
    "        #(\"2r4r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\", # good\n",
    "        #\"3r3r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\"), # bad\n",
    "\n",
    "        # mega blunder\n",
    "        # 2n1kb1r/r2n1ppp/1R2p3/p2pP1N1/Q2P3q/1N2B3/P4PPP/1R4K1 w k - 1 18\n",
    "        (\"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4r3/R2N1PPP/2N1KB1R w K - 0 18\", # good\n",
    "        \"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4P3/R2N1PPP/1rN1KB1R w K - 2 18\"), # bad\n",
    "\n",
    "        # inaccuracy\n",
    "        # r2qkb1r/1pp1nppp/p1n1p3/1B1pPb2/3P4/5N2/PPP2PPP/RNBQ1RK1 w kq - 0 7\n",
    "        (\"rnbq1rk1/ppp2ppp/5n2/3p4/3PpB2/P1b1P3/1PP1NPPP/R2QKB1R w KQ - 0 7\", # good\n",
    "        \"rnbq1rk1/ppp1bppp/5n2/3p4/3PpB2/P1N1P3/1PP1NPPP/R2QKB1R w KQ - 1 7\") # bad\n",
    "    ]\n",
    "\n",
    "    import chess\n",
    "    for (i, (good_fen, bad_fen)) in enumerate(samples):\n",
    "        good_board = chess.Board(good_fen)\n",
    "        bad_board = chess.Board(bad_fen)\n",
    "        pred = chess_model.predict(tf.concat([encode_board(good_board), encode_board(bad_board)], axis=0), verbose=0)\n",
    "        logs[f\"diff{i}\"] = pred[0][0] - pred[1][0] # positive diff = training is good\n",
    "\n",
    "    callbacks.on_epoch_end(epoch, logs)\n",
    "\n",
    "    #lr_schedule = ReduceLROnPlateau(\n",
    "    #    monitor='loss',\n",
    "    #    factor=0.7,\n",
    "    #    patience=15,\n",
    "    #    min_delta=0.001,\n",
    "    #)\n",
    "    if prev_loss - epoch_loss >= 0.0001:\n",
    "        # improvement!\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    prev_loss = epoch_loss\n",
    "    if epochs_without_improvement >= 15:\n",
    "        keras.backend.set_value(optimizer.learning_rate, optimizer.learning_rate.numpy() * 0.7)\n",
    "        epochs_without_improvement = 0\n",
    "callbacks.on_train_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "b = chess.Board(\"r2qkb1r/1pp1nppp/p1n1p3/3pPb2/3P4/5N2/PPP1BPPP/RNBQ1RK1 b kq - 1 7\").mirror()\n",
    "print(b.fen())\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_model = keras.models.load_model(\"/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/models/model-0426-1.775.keras\", compile=False)\n",
    "\n",
    "# NOTE: GOOD/BAD based on the POV of WHO JUST PLAYED\n",
    "samples = [\n",
    "    # 2kr3r/1pp1pp1p/1p6/q4bP1/2B5/4BP2/Pb1NQK1P/R6R w - - 0 18\n",
    "    #(\"2r4r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\", # good\n",
    "    #\"3r3r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\"), # bad\n",
    "\n",
    "    # mega blunder\n",
    "    # 2n1kb1r/r2n1ppp/1R2p3/p2pP1N1/Q2P3q/1N2B3/P4PPP/1R4K1 w k - 1 18\n",
    "    (\"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4r3/R2N1PPP/2N1KB1R w K - 0 18\", # good\n",
    "    \"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4P3/R2N1PPP/1rN1KB1R w K - 2 18\"), # bad\n",
    "\n",
    "    # inaccuracy\n",
    "    # r2qkb1r/1pp1nppp/p1n1p3/1B1pPb2/3P4/5N2/PPP2PPP/RNBQ1RK1 w kq - 0 7\n",
    "    (\"rnbq1rk1/ppp2ppp/5n2/3p4/3PpB2/P1b1P3/1PP1NPPP/R2QKB1R w KQ - 0 7\", # good\n",
    "    \"rnbq1rk1/ppp1bppp/5n2/3p4/3PpB2/P1N1P3/1PP1NPPP/R2QKB1R w KQ - 1 7\") # bad\n",
    "]\n",
    "\n",
    "import chess\n",
    "for (i, (good_fen, bad_fen)) in enumerate(samples):\n",
    "    good_board = chess.Board(good_fen)\n",
    "    bad_board = chess.Board(bad_fen)\n",
    "    pred = chess_model.predict(tf.concat([encode_board(good_board), encode_board(bad_board)], axis=0), verbose=0)\n",
    "    \n",
    "    print(pred[0][0] - pred[1][0]) # good - bad should be +\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
