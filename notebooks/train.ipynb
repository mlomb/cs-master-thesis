{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print([dev.name for dev in device_lib.list_local_devices()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from glob import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1774212.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(\"../data/dataset/2018_1600.bin\")\n",
    "record_size = 3 * 12 * 8\n",
    "batch_size = 4096\n",
    "dataset_size = sum([os.path.getsize(f) for f in files]) / record_size\n",
    "batches_per_epoch = math.ceil(dataset_size / batch_size)\n",
    "\n",
    "dataset = tf.data.FixedLengthRecordDataset(filenames=files, record_bytes=record_size)\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.apply(tf.data.experimental.copy_to_device('/gpu:0'))\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "#tfds.benchmark(dataset, batch_size=batch_size)\n",
    "\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CallbackList\n",
    "from lib.encoding import decode_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12)]              0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda  (None, 12, 1)             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.bitwise.bitwise_and (TF  (None, 12, 64)            0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " tf.math.not_equal (TFOpLam  (None, 12, 64)            0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.cast (TFOpLambda)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328705 (1.25 MB)\n",
      "Trainable params: 328705 (1.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def custom_loss(y_pred):\n",
    "    \"\"\"\n",
    "    Compute loss as defined in https://erikbern.com/2014/11/29/deep-learning-for-chess.html\n",
    "    // sum(p,q,r)logS(f(q)−f(r))+K*log(f(p)+f(q))+K*log(−f(q)−f(p))\n",
    "    \"\"\"\n",
    "    p = y_pred[0]\n",
    "    q = y_pred[1]\n",
    "    r = y_pred[2]\n",
    "    K = 10.0\n",
    "\n",
    "    a = - tf.math.log(tf.math.sigmoid(q - r))\n",
    "    b = - K * tf.math.log(tf.math.sigmoid(p + q))\n",
    "    c = - K * tf.math.log(tf.math.sigmoid(-q - p))\n",
    "\n",
    "    return a + b + c\n",
    "\n",
    "def make_chess_model():\n",
    "    inp = tf.keras.Input(shape=(12,), dtype=tf.int64)\n",
    "    x = decode_board(inp) # convert 12 ints to 768 floats\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(1)(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "chess_model = make_chess_model()\n",
    "chess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/9999   0%|          | 0/434 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706379196.662089    7496 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1/9999 100%|██████████| 434/434 [00:07<00:00, 61.99it/s, loss=14.7811 loss_batch=14.5602] \n",
      "Epoch 2/9999 100%|██████████| 434/434 [00:03<00:00, 113.17it/s, loss=14.5597 loss_batch=14.5571]\n",
      "Epoch 3/9999 100%|██████████| 434/434 [00:04<00:00, 106.74it/s, loss=14.5633 loss_batch=14.5484]\n",
      "Epoch 4/9999 100%|██████████| 434/434 [00:04<00:00, 102.97it/s, loss=14.6104 loss_batch=14.5561]\n",
      "Epoch 5/9999 100%|██████████| 434/434 [00:03<00:00, 110.44it/s, loss=14.5514 loss_batch=14.5561]\n",
      "Epoch 6/9999 100%|██████████| 434/434 [00:03<00:00, 109.83it/s, loss=14.5662 loss_batch=14.5561]\n",
      "Epoch 7/9999 100%|██████████| 434/434 [00:03<00:00, 109.82it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 8/9999 100%|██████████| 434/434 [00:03<00:00, 109.52it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 9/9999 100%|██████████| 434/434 [00:03<00:00, 113.43it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 10/9999 100%|██████████| 434/434 [00:03<00:00, 113.89it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 11/9999 100%|██████████| 434/434 [00:03<00:00, 112.58it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 12/9999 100%|██████████| 434/434 [00:04<00:00, 103.54it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 13/9999 100%|██████████| 434/434 [00:03<00:00, 109.36it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 14/9999 100%|██████████| 434/434 [00:03<00:00, 111.44it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 15/9999 100%|██████████| 434/434 [00:05<00:00, 84.67it/s, loss=14.5545 loss_batch=14.5561] \n",
      "Epoch 16/9999 100%|██████████| 434/434 [00:03<00:00, 115.20it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 17/9999 100%|██████████| 434/434 [00:03<00:00, 110.80it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 18/9999 100%|██████████| 434/434 [00:03<00:00, 112.59it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 19/9999 100%|██████████| 434/434 [00:05<00:00, 84.65it/s, loss=14.5545 loss_batch=14.5561] \n",
      "Epoch 20/9999 100%|██████████| 434/434 [00:03<00:00, 113.41it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 21/9999 100%|██████████| 434/434 [00:03<00:00, 109.37it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 22/9999 100%|██████████| 434/434 [00:03<00:00, 115.46it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 23/9999 100%|██████████| 434/434 [00:03<00:00, 113.61it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 24/9999 100%|██████████| 434/434 [00:04<00:00, 108.24it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 25/9999 100%|██████████| 434/434 [00:03<00:00, 113.67it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 26/9999 100%|██████████| 434/434 [00:03<00:00, 114.64it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 27/9999 100%|██████████| 434/434 [00:03<00:00, 114.35it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 28/9999 100%|██████████| 434/434 [00:03<00:00, 114.85it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 29/9999 100%|██████████| 434/434 [00:03<00:00, 112.10it/s, loss=14.5545 loss_batch=14.5561]\n",
      "Epoch 30/9999 100%|██████████| 434/434 [00:03<00:00, 114.39it/s, loss=14.5545 loss_batch=14.5562]\n",
      "Epoch 31/9999  12%|█▏        | 50/434 [00:00<00:03, 108.07it/s, loss=14.5561 loss_batch=14.5561]"
     ]
    }
   ],
   "source": [
    "ts = int(time())\n",
    "epochs = 9999\n",
    "#optimizer = SGD(learning_rate=0.03, nesterov=True, momentum=0.9, clipnorm=1)\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "callbacks = CallbackList([\n",
    "    ModelCheckpoint(f\"checkpoints/{ts}\" + \"/model-{epoch:02d}-{loss:.2f}.keras\"),\n",
    "    TensorBoard(log_dir=f\"./logs/{ts}\", write_graph=False)    \n",
    "], model=chess_model)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    batch = tf.reshape(tf.io.decode_raw(batch, tf.int64), (-1, 3, 12))\n",
    "\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = tf.reshape(chess_model(tf.reshape(batch, (-1, 12)), training=True), (-1, 3))  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value = custom_loss(logits)\n",
    "    \n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(loss_value, chess_model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, chess_model.trainable_weights))\n",
    "\n",
    "    # Update metrics\n",
    "    loss_tracker.update_state(loss_value[0])\n",
    "\n",
    "    return loss_value[0]\n",
    "\n",
    "callbacks.on_train_begin()\n",
    "for epoch in range(epochs):\n",
    "    loss_tracker.reset_states()\n",
    "    callbacks.on_epoch_begin(epoch)\n",
    "\n",
    "    batch_i = 0\n",
    "    batch_i_last = 0\n",
    "    with tqdm(total=batches_per_epoch, bar_format=f\"Epoch {epoch+1}/{epochs}\" + \" {l_bar}{bar:10}{r_bar}{bar:-10b}\") as pbar:\n",
    "        for batch in dataset:\n",
    "            loss_value = train_step(batch)\n",
    "\n",
    "            batch_i += 1\n",
    "            if batch_i % 10 == 0 or batch_i == batches_per_epoch:\n",
    "                pbar.set_postfix_str(f\"loss={loss_tracker.result():.4f} loss_batch={float(loss_value):.4f}\")\n",
    "                pbar.update(batch_i - batch_i_last)\n",
    "                batch_i_last = batch_i\n",
    "    \n",
    "    callbacks.on_epoch_end(epoch, logs={\"loss\": loss_tracker.result()})\n",
    "callbacks.on_train_end()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
