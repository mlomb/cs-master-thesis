{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print([dev.name for dev in device_lib.list_local_devices()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from glob import glob\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266654.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob(\"../data/dataset/2013_1600.bin\")\n",
    "record_size = 3 * 12 * 8\n",
    "batch_size = 4096\n",
    "dataset_size = sum([os.path.getsize(f) for f in files]) / record_size\n",
    "batches_per_epoch = math.ceil(dataset_size / batch_size)\n",
    "\n",
    "dataset = tf.data.FixedLengthRecordDataset(filenames=files, record_bytes=record_size)\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.apply(tf.data.experimental.copy_to_device('/gpu:0'))\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#import tensorflow_datasets as tfds\n",
    "#tfds.benchmark(dataset, batch_size=batch_size)\n",
    "\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CallbackList\n",
    "from lib.encoding import encode_board, decode_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12)]              0         \n",
      "                                                                 \n",
      " tf.expand_dims (TFOpLambda  (None, 12, 1)             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.bitwise.bitwise_and (TF  (None, 12, 64)            0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " tf.math.not_equal (TFOpLam  (None, 12, 64)            0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.cast (TFOpLambda)        (None, 12, 64)            0         \n",
      "                                                                 \n",
      " tf.reshape (TFOpLambda)     (None, 768)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               196864    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328705 (1.25 MB)\n",
      "Trainable params: 328705 (1.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def custom_loss(y_pred):\n",
    "    \"\"\"\n",
    "    Compute loss as defined in https://erikbern.com/2014/11/29/deep-learning-for-chess.html\n",
    "    // sum(p,q,r)logS(f(q)−f(r))+K*log(f(p)+f(q))+K*log(−f(q)−f(p))\n",
    "    \"\"\"\n",
    "    p = y_pred[:,0]\n",
    "    q = y_pred[:,1]\n",
    "    r = y_pred[:,2]\n",
    "    K = 1.0\n",
    "\n",
    "    rq_diff = q - r\n",
    "    pq_diff = K * (p + q)\n",
    "\n",
    "    a = - tf.math.reduce_mean(tf.math.log(tf.math.sigmoid(rq_diff)))\n",
    "    b = - tf.math.reduce_mean(tf.math.log(tf.math.sigmoid( pq_diff)))\n",
    "    c = - tf.math.reduce_mean(tf.math.log(tf.math.sigmoid(-pq_diff)))\n",
    "\n",
    "    reg = 0.0 # L2\n",
    "    for x in chess_model.trainable_variables:\n",
    "        reg += 0.01 * tf.math.reduce_mean(tf.math.square(x))\n",
    "\n",
    "    loss = a + b + c\n",
    "    obj = loss + reg\n",
    "\n",
    "    return loss, obj\n",
    "\n",
    "def make_chess_model():\n",
    "    inp = tf.keras.Input(shape=(12,), dtype=tf.int64)\n",
    "    x = decode_board(inp) # convert 12 ints to 768 floats\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dense(1)(x)\n",
    "    return Model(inp, x)\n",
    "\n",
    "chess_model = make_chess_model()\n",
    "chess_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/99999   0%|          | 0/66 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1706587110.626778   27288 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "Epoch 1/99999 100%|██████████| 66/66 [00:05<00:00, 11.62it/s, loss=2.1724 obj=2.1727 lr=0.0100]\n",
      "Epoch 2/99999 100%|██████████| 66/66 [00:01<00:00, 60.35it/s, loss=1.9913 obj=1.9917 lr=0.0100]\n",
      "Epoch 3/99999 100%|██████████| 66/66 [00:01<00:00, 59.48it/s, loss=1.9749 obj=1.9753 lr=0.0100]\n",
      "Epoch 4/99999 100%|██████████| 66/66 [00:00<00:00, 67.75it/s, loss=1.9610 obj=1.9614 lr=0.0100]\n",
      "Epoch 5/99999 100%|██████████| 66/66 [00:00<00:00, 80.21it/s, loss=1.9565 obj=1.9569 lr=0.0100]\n",
      "Epoch 6/99999 100%|██████████| 66/66 [00:00<00:00, 82.55it/s, loss=1.9472 obj=1.9476 lr=0.0100]\n",
      "Epoch 7/99999 100%|██████████| 66/66 [00:00<00:00, 78.42it/s, loss=1.9406 obj=1.9411 lr=0.0100]\n",
      "Epoch 8/99999 100%|██████████| 66/66 [00:00<00:00, 78.65it/s, loss=1.9357 obj=1.9362 lr=0.0100]\n",
      "Epoch 9/99999 100%|██████████| 66/66 [00:00<00:00, 69.72it/s, loss=1.9286 obj=1.9291 lr=0.0100]\n",
      "Epoch 10/99999 100%|██████████| 66/66 [00:01<00:00, 63.18it/s, loss=1.9267 obj=1.9273 lr=0.0100]\n",
      "Epoch 11/99999 100%|██████████| 66/66 [00:01<00:00, 65.42it/s, loss=1.9196 obj=1.9203 lr=0.0100]\n",
      "Epoch 12/99999 100%|██████████| 66/66 [00:00<00:00, 72.50it/s, loss=1.9151 obj=1.9158 lr=0.0100]\n",
      "Epoch 13/99999 100%|██████████| 66/66 [00:01<00:00, 57.44it/s, loss=1.9285 obj=1.9293 lr=0.0100]\n",
      "Epoch 14/99999 100%|██████████| 66/66 [00:00<00:00, 75.08it/s, loss=1.9103 obj=1.9112 lr=0.0100]\n",
      "Epoch 15/99999 100%|██████████| 66/66 [00:00<00:00, 82.71it/s, loss=1.9035 obj=1.9045 lr=0.0100]\n",
      "Epoch 16/99999 100%|██████████| 66/66 [00:00<00:00, 82.58it/s, loss=1.9038 obj=1.9048 lr=0.0100]\n",
      "Epoch 17/99999 100%|██████████| 66/66 [00:00<00:00, 80.07it/s, loss=1.9005 obj=1.9016 lr=0.0100]\n",
      "Epoch 18/99999 100%|██████████| 66/66 [00:00<00:00, 78.24it/s, loss=1.8915 obj=1.8927 lr=0.0100]\n",
      "Epoch 19/99999 100%|██████████| 66/66 [00:00<00:00, 81.10it/s, loss=1.8881 obj=1.8895 lr=0.0100]\n",
      "Epoch 20/99999 100%|██████████| 66/66 [00:00<00:00, 75.67it/s, loss=1.8863 obj=1.8877 lr=0.0100]\n",
      "Epoch 21/99999 100%|██████████| 66/66 [00:01<00:00, 64.63it/s, loss=1.8890 obj=1.8905 lr=0.0100]\n",
      "Epoch 22/99999 100%|██████████| 66/66 [00:00<00:00, 72.50it/s, loss=1.8816 obj=1.8831 lr=0.0100]\n",
      "Epoch 23/99999 100%|██████████| 66/66 [00:00<00:00, 76.53it/s, loss=1.8782 obj=1.8798 lr=0.0100]\n",
      "Epoch 24/99999 100%|██████████| 66/66 [00:00<00:00, 82.19it/s, loss=1.8726 obj=1.8743 lr=0.0100]\n",
      "Epoch 25/99999 100%|██████████| 66/66 [00:00<00:00, 82.94it/s, loss=1.8772 obj=1.8790 lr=0.0100]\n",
      "Epoch 26/99999 100%|██████████| 66/66 [00:00<00:00, 82.72it/s, loss=1.8735 obj=1.8754 lr=0.0100]\n",
      "Epoch 27/99999 100%|██████████| 66/66 [00:00<00:00, 82.50it/s, loss=1.8673 obj=1.8693 lr=0.0100]\n",
      "Epoch 28/99999 100%|██████████| 66/66 [00:00<00:00, 82.11it/s, loss=1.8702 obj=1.8723 lr=0.0100]\n",
      "Epoch 29/99999 100%|██████████| 66/66 [00:00<00:00, 77.89it/s, loss=1.8638 obj=1.8660 lr=0.0100]\n",
      "Epoch 30/99999 100%|██████████| 66/66 [00:00<00:00, 83.08it/s, loss=1.8596 obj=1.8619 lr=0.0100]\n",
      "Epoch 31/99999 100%|██████████| 66/66 [00:00<00:00, 79.84it/s, loss=1.8575 obj=1.8598 lr=0.0100]\n",
      "Epoch 32/99999 100%|██████████| 66/66 [00:00<00:00, 76.16it/s, loss=1.8551 obj=1.8576 lr=0.0100]\n",
      "Epoch 33/99999 100%|██████████| 66/66 [00:00<00:00, 73.80it/s, loss=1.8531 obj=1.8556 lr=0.0100]\n",
      "Epoch 34/99999 100%|██████████| 66/66 [00:00<00:00, 80.35it/s, loss=1.8539 obj=1.8565 lr=0.0100]\n",
      "Epoch 35/99999 100%|██████████| 66/66 [00:00<00:00, 79.89it/s, loss=1.8517 obj=1.8545 lr=0.0100]\n",
      "Epoch 36/99999 100%|██████████| 66/66 [00:00<00:00, 77.74it/s, loss=1.8497 obj=1.8526 lr=0.0100]\n",
      "Epoch 37/99999 100%|██████████| 66/66 [00:00<00:00, 67.52it/s, loss=1.8475 obj=1.8505 lr=0.0100]\n",
      "Epoch 38/99999 100%|██████████| 66/66 [00:00<00:00, 68.11it/s, loss=1.8513 obj=1.8543 lr=0.0100]\n",
      "Epoch 39/99999 100%|██████████| 66/66 [00:00<00:00, 73.64it/s, loss=1.8480 obj=1.8512 lr=0.0100]\n",
      "Epoch 40/99999 100%|██████████| 66/66 [00:00<00:00, 71.72it/s, loss=1.8428 obj=1.8460 lr=0.0100]\n",
      "Epoch 41/99999 100%|██████████| 66/66 [00:00<00:00, 79.61it/s, loss=1.8384 obj=1.8417 lr=0.0100]\n",
      "Epoch 42/99999 100%|██████████| 66/66 [00:00<00:00, 78.62it/s, loss=1.8348 obj=1.8382 lr=0.0100]\n",
      "Epoch 43/99999 100%|██████████| 66/66 [00:00<00:00, 81.99it/s, loss=1.8327 obj=1.8363 lr=0.0100]\n",
      "Epoch 44/99999 100%|██████████| 66/66 [00:00<00:00, 82.04it/s, loss=1.8336 obj=1.8372 lr=0.0100]\n",
      "Epoch 45/99999 100%|██████████| 66/66 [00:00<00:00, 81.69it/s, loss=1.8344 obj=1.8382 lr=0.0100]\n",
      "Epoch 46/99999 100%|██████████| 66/66 [00:00<00:00, 81.97it/s, loss=1.8326 obj=1.8365 lr=0.0100]\n",
      "Epoch 47/99999 100%|██████████| 66/66 [00:00<00:00, 68.86it/s, loss=1.8325 obj=1.8364 lr=0.0100]\n",
      "Epoch 48/99999 100%|██████████| 66/66 [00:00<00:00, 75.24it/s, loss=1.8297 obj=1.8338 lr=0.0100]\n",
      "Epoch 49/99999 100%|██████████| 66/66 [00:00<00:00, 71.68it/s, loss=1.8293 obj=1.8334 lr=0.0100]\n",
      "Epoch 50/99999 100%|██████████| 66/66 [00:00<00:00, 80.04it/s, loss=1.8302 obj=1.8344 lr=0.0100]\n",
      "Epoch 51/99999 100%|██████████| 66/66 [00:00<00:00, 79.40it/s, loss=1.8255 obj=1.8298 lr=0.0100]\n",
      "Epoch 52/99999 100%|██████████| 66/66 [00:00<00:00, 82.74it/s, loss=1.8253 obj=1.8297 lr=0.0100]\n",
      "Epoch 53/99999 100%|██████████| 66/66 [00:00<00:00, 81.81it/s, loss=1.8203 obj=1.8249 lr=0.0100]\n",
      "Epoch 54/99999 100%|██████████| 66/66 [00:00<00:00, 81.11it/s, loss=1.8242 obj=1.8288 lr=0.0100]\n",
      "Epoch 55/99999 100%|██████████| 66/66 [00:00<00:00, 75.33it/s, loss=1.8248 obj=1.8295 lr=0.0100]\n",
      "Epoch 56/99999 100%|██████████| 66/66 [00:00<00:00, 68.21it/s, loss=1.8158 obj=1.8206 lr=0.0100]\n",
      "Epoch 57/99999 100%|██████████| 66/66 [00:00<00:00, 76.81it/s, loss=1.8118 obj=1.8167 lr=0.0100]\n",
      "Epoch 58/99999 100%|██████████| 66/66 [00:00<00:00, 77.90it/s, loss=1.8097 obj=1.8148 lr=0.0100]\n",
      "Epoch 59/99999 100%|██████████| 66/66 [00:00<00:00, 69.16it/s, loss=1.8093 obj=1.8144 lr=0.0100]\n",
      "Epoch 60/99999 100%|██████████| 66/66 [00:00<00:00, 79.40it/s, loss=1.8102 obj=1.8155 lr=0.0100]\n",
      "Epoch 61/99999 100%|██████████| 66/66 [00:00<00:00, 77.87it/s, loss=1.8075 obj=1.8128 lr=0.0100]\n",
      "Epoch 62/99999 100%|██████████| 66/66 [00:00<00:00, 79.13it/s, loss=1.8080 obj=1.8135 lr=0.0100]\n",
      "Epoch 63/99999 100%|██████████| 66/66 [00:01<00:00, 62.39it/s, loss=1.8051 obj=1.8107 lr=0.0100]\n",
      "Epoch 64/99999 100%|██████████| 66/66 [00:00<00:00, 78.67it/s, loss=1.8000 obj=1.8056 lr=0.0100]\n",
      "Epoch 65/99999 100%|██████████| 66/66 [00:00<00:00, 72.51it/s, loss=1.7972 obj=1.8030 lr=0.0100]\n",
      "Epoch 66/99999 100%|██████████| 66/66 [00:00<00:00, 75.41it/s, loss=1.7968 obj=1.8027 lr=0.0100]\n",
      "Epoch 67/99999 100%|██████████| 66/66 [00:00<00:00, 68.15it/s, loss=1.7947 obj=1.8006 lr=0.0100]\n",
      "Epoch 68/99999 100%|██████████| 66/66 [00:01<00:00, 65.21it/s, loss=1.7939 obj=1.7999 lr=0.0100]\n",
      "Epoch 69/99999 100%|██████████| 66/66 [00:01<00:00, 65.31it/s, loss=1.7930 obj=1.7992 lr=0.0100]\n",
      "Epoch 70/99999 100%|██████████| 66/66 [00:00<00:00, 79.07it/s, loss=1.7921 obj=1.7984 lr=0.0100]\n",
      "Epoch 71/99999 100%|██████████| 66/66 [00:00<00:00, 75.04it/s, loss=1.7914 obj=1.7978 lr=0.0100]\n",
      "Epoch 72/99999 100%|██████████| 66/66 [00:00<00:00, 77.07it/s, loss=1.7920 obj=1.7985 lr=0.0100]\n",
      "Epoch 73/99999 100%|██████████| 66/66 [00:00<00:00, 80.42it/s, loss=1.7899 obj=1.7965 lr=0.0100]\n",
      "Epoch 74/99999 100%|██████████| 66/66 [00:00<00:00, 68.37it/s, loss=1.7909 obj=1.7977 lr=0.0100]\n",
      "Epoch 75/99999 100%|██████████| 66/66 [00:00<00:00, 81.71it/s, loss=1.7921 obj=1.7990 lr=0.0100]\n",
      "Epoch 76/99999 100%|██████████| 66/66 [00:00<00:00, 73.76it/s, loss=1.7953 obj=1.8022 lr=0.0100]\n",
      "Epoch 77/99999 100%|██████████| 66/66 [00:01<00:00, 60.54it/s, loss=1.7927 obj=1.7997 lr=0.0100]\n",
      "Epoch 78/99999 100%|██████████| 66/66 [00:00<00:00, 70.53it/s, loss=1.7831 obj=1.7902 lr=0.0100]\n",
      "Epoch 79/99999 100%|██████████| 66/66 [00:00<00:00, 77.83it/s, loss=1.7806 obj=1.7878 lr=0.0100]\n",
      "Epoch 80/99999 100%|██████████| 66/66 [00:00<00:00, 77.17it/s, loss=1.7802 obj=1.7876 lr=0.0100]\n",
      "Epoch 81/99999 100%|██████████| 66/66 [00:00<00:00, 66.57it/s, loss=1.7805 obj=1.7879 lr=0.0100]\n",
      "Epoch 82/99999 100%|██████████| 66/66 [00:01<00:00, 40.60it/s, loss=1.7792 obj=1.7867 lr=0.0100]\n",
      "Epoch 83/99999 100%|██████████| 66/66 [00:00<00:00, 68.54it/s, loss=1.7841 obj=1.7918 lr=0.0100]\n",
      "Epoch 84/99999 100%|██████████| 66/66 [00:01<00:00, 57.85it/s, loss=1.7835 obj=1.7913 lr=0.0100]\n",
      "Epoch 85/99999 100%|██████████| 66/66 [00:00<00:00, 66.10it/s, loss=1.7808 obj=1.7886 lr=0.0100]\n",
      "Epoch 86/99999 100%|██████████| 66/66 [00:00<00:00, 74.10it/s, loss=1.7749 obj=1.7829 lr=0.0100]\n",
      "Epoch 87/99999 100%|██████████| 66/66 [00:00<00:00, 71.22it/s, loss=1.7726 obj=1.7807 lr=0.0100]\n",
      "Epoch 88/99999 100%|██████████| 66/66 [00:00<00:00, 71.16it/s, loss=1.7736 obj=1.7817 lr=0.0100]\n",
      "Epoch 89/99999 100%|██████████| 66/66 [00:00<00:00, 72.70it/s, loss=1.7695 obj=1.7777 lr=0.0100]\n",
      "Epoch 90/99999 100%|██████████| 66/66 [00:00<00:00, 71.33it/s, loss=1.7706 obj=1.7790 lr=0.0100]\n",
      "Epoch 91/99999 100%|██████████| 66/66 [00:00<00:00, 68.25it/s, loss=1.7688 obj=1.7772 lr=0.0100]\n",
      "Epoch 92/99999 100%|██████████| 66/66 [00:00<00:00, 72.70it/s, loss=1.7684 obj=1.7769 lr=0.0100]\n",
      "Epoch 93/99999 100%|██████████| 66/66 [00:00<00:00, 79.94it/s, loss=1.7662 obj=1.7748 lr=0.0100]\n",
      "Epoch 94/99999 100%|██████████| 66/66 [00:00<00:00, 78.54it/s, loss=1.7676 obj=1.7763 lr=0.0100]\n",
      "Epoch 95/99999 100%|██████████| 66/66 [00:00<00:00, 72.43it/s, loss=1.7643 obj=1.7731 lr=0.0100]\n",
      "Epoch 96/99999 100%|██████████| 66/66 [00:00<00:00, 74.61it/s, loss=1.7684 obj=1.7773 lr=0.0100]\n",
      "Epoch 97/99999 100%|██████████| 66/66 [00:00<00:00, 77.66it/s, loss=1.7718 obj=1.7808 lr=0.0100]\n",
      "Epoch 98/99999 100%|██████████| 66/66 [00:00<00:00, 75.20it/s, loss=1.7695 obj=1.7786 lr=0.0100]\n",
      "Epoch 99/99999 100%|██████████| 66/66 [00:00<00:00, 71.73it/s, loss=1.7693 obj=1.7785 lr=0.0100]\n",
      "Epoch 100/99999 100%|██████████| 66/66 [00:00<00:00, 68.05it/s, loss=1.7665 obj=1.7758 lr=0.0100]\n",
      "Epoch 101/99999 100%|██████████| 66/66 [00:01<00:00, 59.70it/s, loss=1.7632 obj=1.7726 lr=0.0100]\n",
      "Epoch 102/99999 100%|██████████| 66/66 [00:01<00:00, 64.65it/s, loss=1.7581 obj=1.7676 lr=0.0100]\n",
      "Epoch 103/99999 100%|██████████| 66/66 [00:01<00:00, 64.48it/s, loss=1.7554 obj=1.7649 lr=0.0100]\n",
      "Epoch 104/99999 100%|██████████| 66/66 [00:00<00:00, 71.14it/s, loss=1.7550 obj=1.7646 lr=0.0100]\n",
      "Epoch 105/99999 100%|██████████| 66/66 [00:00<00:00, 76.27it/s, loss=1.7550 obj=1.7647 lr=0.0100]\n",
      "Epoch 106/99999 100%|██████████| 66/66 [00:00<00:00, 75.57it/s, loss=1.7557 obj=1.7655 lr=0.0100]\n",
      "Epoch 107/99999 100%|██████████| 66/66 [00:00<00:00, 72.47it/s, loss=1.7552 obj=1.7651 lr=0.0100]\n",
      "Epoch 108/99999 100%|██████████| 66/66 [00:00<00:00, 71.03it/s, loss=1.7556 obj=1.7656 lr=0.0100]\n",
      "Epoch 109/99999 100%|██████████| 66/66 [00:01<00:00, 65.43it/s, loss=1.7538 obj=1.7639 lr=0.0100]\n",
      "Epoch 110/99999 100%|██████████| 66/66 [00:00<00:00, 73.29it/s, loss=1.7541 obj=1.7643 lr=0.0100]\n",
      "Epoch 111/99999 100%|██████████| 66/66 [00:00<00:00, 76.93it/s, loss=1.7532 obj=1.7635 lr=0.0100]\n",
      "Epoch 112/99999 100%|██████████| 66/66 [00:00<00:00, 69.89it/s, loss=1.7567 obj=1.7671 lr=0.0100]\n",
      "Epoch 113/99999 100%|██████████| 66/66 [00:01<00:00, 64.64it/s, loss=1.7570 obj=1.7674 lr=0.0100]\n",
      "Epoch 114/99999 100%|██████████| 66/66 [00:00<00:00, 75.26it/s, loss=1.7528 obj=1.7634 lr=0.0100]\n",
      "Epoch 115/99999 100%|██████████| 66/66 [00:00<00:00, 66.74it/s, loss=1.7511 obj=1.7618 lr=0.0100]\n",
      "Epoch 116/99999 100%|██████████| 66/66 [00:00<00:00, 68.17it/s, loss=1.7487 obj=1.7595 lr=0.0100]\n",
      "Epoch 117/99999 100%|██████████| 66/66 [00:00<00:00, 66.58it/s, loss=1.7485 obj=1.7594 lr=0.0100]\n",
      "Epoch 118/99999 100%|██████████| 66/66 [00:00<00:00, 78.77it/s, loss=1.7486 obj=1.7595 lr=0.0100]\n",
      "Epoch 119/99999 100%|██████████| 66/66 [00:00<00:00, 68.09it/s, loss=1.7524 obj=1.7634 lr=0.0100]\n",
      "Epoch 120/99999 100%|██████████| 66/66 [00:00<00:00, 70.45it/s, loss=1.7531 obj=1.7642 lr=0.0100]\n",
      "Epoch 121/99999 100%|██████████| 66/66 [00:00<00:00, 75.08it/s, loss=1.7499 obj=1.7611 lr=0.0100]\n",
      "Epoch 122/99999 100%|██████████| 66/66 [00:00<00:00, 69.94it/s, loss=1.7452 obj=1.7564 lr=0.0100]\n",
      "Epoch 123/99999 100%|██████████| 66/66 [00:00<00:00, 77.76it/s, loss=1.7448 obj=1.7561 lr=0.0100]\n",
      "Epoch 124/99999 100%|██████████| 66/66 [00:00<00:00, 69.81it/s, loss=1.7438 obj=1.7552 lr=0.0100]\n",
      "Epoch 125/99999 100%|██████████| 66/66 [00:00<00:00, 72.17it/s, loss=1.7430 obj=1.7545 lr=0.0100]\n",
      "Epoch 126/99999 100%|██████████| 66/66 [00:00<00:00, 68.14it/s, loss=1.7423 obj=1.7539 lr=0.0100]\n",
      "Epoch 127/99999 100%|██████████| 66/66 [00:00<00:00, 72.95it/s, loss=1.7395 obj=1.7512 lr=0.0100]\n",
      "Epoch 128/99999 100%|██████████| 66/66 [00:01<00:00, 63.32it/s, loss=1.7411 obj=1.7528 lr=0.0100]\n",
      "Epoch 129/99999 100%|██████████| 66/66 [00:01<00:00, 62.43it/s, loss=1.7441 obj=1.7559 lr=0.0100]\n",
      "Epoch 130/99999 100%|██████████| 66/66 [00:01<00:00, 61.14it/s, loss=1.7459 obj=1.7578 lr=0.0100]\n",
      "Epoch 131/99999 100%|██████████| 66/66 [00:00<00:00, 74.98it/s, loss=1.7476 obj=1.7596 lr=0.0100]\n",
      "Epoch 132/99999 100%|██████████| 66/66 [00:00<00:00, 67.77it/s, loss=1.7420 obj=1.7540 lr=0.0100]\n",
      "Epoch 133/99999 100%|██████████| 66/66 [00:00<00:00, 75.29it/s, loss=1.7390 obj=1.7512 lr=0.0100]\n",
      "Epoch 134/99999 100%|██████████| 66/66 [00:00<00:00, 73.07it/s, loss=1.7384 obj=1.7506 lr=0.0100]\n",
      "Epoch 135/99999 100%|██████████| 66/66 [00:00<00:00, 70.38it/s, loss=1.7382 obj=1.7504 lr=0.0100]\n",
      "Epoch 136/99999 100%|██████████| 66/66 [00:00<00:00, 72.55it/s, loss=1.7367 obj=1.7491 lr=0.0100]\n",
      "Epoch 137/99999 100%|██████████| 66/66 [00:00<00:00, 74.59it/s, loss=1.7364 obj=1.7488 lr=0.0100]\n",
      "Epoch 138/99999 100%|██████████| 66/66 [00:00<00:00, 73.08it/s, loss=1.7358 obj=1.7483 lr=0.0100]\n",
      "Epoch 139/99999 100%|██████████| 66/66 [00:00<00:00, 71.40it/s, loss=1.7380 obj=1.7506 lr=0.0100]\n",
      "Epoch 140/99999 100%|██████████| 66/66 [00:00<00:00, 73.63it/s, loss=1.7429 obj=1.7556 lr=0.0100]\n",
      "Epoch 141/99999 100%|██████████| 66/66 [00:00<00:00, 70.00it/s, loss=1.7381 obj=1.7509 lr=0.0100]\n",
      "Epoch 142/99999 100%|██████████| 66/66 [00:00<00:00, 66.55it/s, loss=1.7347 obj=1.7475 lr=0.0100]\n",
      "Epoch 143/99999 100%|██████████| 66/66 [00:00<00:00, 70.01it/s, loss=1.7344 obj=1.7473 lr=0.0100]\n",
      "Epoch 144/99999 100%|██████████| 66/66 [00:00<00:00, 74.82it/s, loss=1.7330 obj=1.7460 lr=0.0100]\n",
      "Epoch 145/99999 100%|██████████| 66/66 [00:00<00:00, 72.13it/s, loss=1.7335 obj=1.7466 lr=0.0100]\n",
      "Epoch 146/99999 100%|██████████| 66/66 [00:00<00:00, 80.53it/s, loss=1.7328 obj=1.7459 lr=0.0100]\n",
      "Epoch 147/99999 100%|██████████| 66/66 [00:00<00:00, 82.22it/s, loss=1.7344 obj=1.7476 lr=0.0100]\n",
      "Epoch 148/99999 100%|██████████| 66/66 [00:00<00:00, 79.71it/s, loss=1.7336 obj=1.7469 lr=0.0100]\n",
      "Epoch 149/99999 100%|██████████| 66/66 [00:00<00:00, 80.17it/s, loss=1.7306 obj=1.7439 lr=0.0100]\n",
      "Epoch 150/99999 100%|██████████| 66/66 [00:00<00:00, 72.94it/s, loss=1.7312 obj=1.7447 lr=0.0100]\n",
      "Epoch 151/99999 100%|██████████| 66/66 [00:00<00:00, 68.03it/s, loss=1.7348 obj=1.7483 lr=0.0100]\n",
      "Epoch 152/99999 100%|██████████| 66/66 [00:00<00:00, 73.56it/s, loss=1.7328 obj=1.7464 lr=0.0100]\n",
      "Epoch 153/99999 100%|██████████| 66/66 [00:00<00:00, 77.02it/s, loss=1.7335 obj=1.7472 lr=0.0100]\n",
      "Epoch 154/99999 100%|██████████| 66/66 [00:00<00:00, 76.20it/s, loss=1.7330 obj=1.7468 lr=0.0100]\n",
      "Epoch 155/99999 100%|██████████| 66/66 [00:01<00:00, 65.43it/s, loss=1.7301 obj=1.7439 lr=0.0100]\n",
      "Epoch 156/99999 100%|██████████| 66/66 [00:00<00:00, 69.49it/s, loss=1.7321 obj=1.7460 lr=0.0100]\n",
      "Epoch 157/99999 100%|██████████| 66/66 [00:00<00:00, 78.31it/s, loss=1.7338 obj=1.7477 lr=0.0100]\n",
      "Epoch 158/99999 100%|██████████| 66/66 [00:00<00:00, 69.84it/s, loss=1.7357 obj=1.7497 lr=0.0100]\n",
      "Epoch 159/99999 100%|██████████| 66/66 [00:00<00:00, 75.54it/s, loss=1.7346 obj=1.7487 lr=0.0100]\n",
      "Epoch 160/99999 100%|██████████| 66/66 [00:00<00:00, 67.59it/s, loss=1.7354 obj=1.7496 lr=0.0100]\n",
      "Epoch 161/99999 100%|██████████| 66/66 [00:00<00:00, 81.42it/s, loss=1.7350 obj=1.7493 lr=0.0100]\n",
      "Epoch 162/99999 100%|██████████| 66/66 [00:00<00:00, 78.67it/s, loss=1.7335 obj=1.7478 lr=0.0100]\n",
      "Epoch 163/99999 100%|██████████| 66/66 [00:00<00:00, 77.25it/s, loss=1.7348 obj=1.7492 lr=0.0100]\n",
      "Epoch 164/99999 100%|██████████| 66/66 [00:00<00:00, 82.11it/s, loss=1.7405 obj=1.7550 lr=0.0100]\n",
      "Epoch 165/99999 100%|██████████| 66/66 [00:00<00:00, 81.92it/s, loss=1.7334 obj=1.7480 lr=0.0100]\n",
      "Epoch 166/99999 100%|██████████| 66/66 [00:00<00:00, 75.56it/s, loss=1.7229 obj=1.7376 lr=0.0100]\n",
      "Epoch 167/99999 100%|██████████| 66/66 [00:00<00:00, 69.60it/s, loss=1.7201 obj=1.7349 lr=0.0100]\n",
      "Epoch 168/99999 100%|██████████| 66/66 [00:00<00:00, 81.64it/s, loss=1.7190 obj=1.7338 lr=0.0100]\n",
      "Epoch 169/99999 100%|██████████| 66/66 [00:00<00:00, 68.96it/s, loss=1.7186 obj=1.7335 lr=0.0100]\n",
      "Epoch 170/99999 100%|██████████| 66/66 [00:00<00:00, 75.89it/s, loss=1.7200 obj=1.7349 lr=0.0100]\n",
      "Epoch 171/99999 100%|██████████| 66/66 [00:01<00:00, 65.38it/s, loss=1.7216 obj=1.7366 lr=0.0100]\n",
      "Epoch 172/99999 100%|██████████| 66/66 [00:00<00:00, 81.17it/s, loss=1.7224 obj=1.7374 lr=0.0100]\n",
      "Epoch 173/99999 100%|██████████| 66/66 [00:00<00:00, 76.80it/s, loss=1.7237 obj=1.7388 lr=0.0100]\n",
      "Epoch 174/99999 100%|██████████| 66/66 [00:00<00:00, 80.29it/s, loss=1.7215 obj=1.7367 lr=0.0100]\n",
      "Epoch 175/99999 100%|██████████| 66/66 [00:00<00:00, 77.43it/s, loss=1.7189 obj=1.7341 lr=0.0100]\n",
      "Epoch 176/99999 100%|██████████| 66/66 [00:00<00:00, 73.69it/s, loss=1.7190 obj=1.7343 lr=0.0100]\n",
      "Epoch 177/99999 100%|██████████| 66/66 [00:00<00:00, 76.98it/s, loss=1.7202 obj=1.7355 lr=0.0100]\n",
      "Epoch 178/99999 100%|██████████| 66/66 [00:00<00:00, 70.82it/s, loss=1.7207 obj=1.7362 lr=0.0100]\n",
      "Epoch 179/99999 100%|██████████| 66/66 [00:00<00:00, 79.53it/s, loss=1.7213 obj=1.7368 lr=0.0100]\n",
      "Epoch 180/99999 100%|██████████| 66/66 [00:00<00:00, 76.77it/s, loss=1.7229 obj=1.7385 lr=0.0100]\n",
      "Epoch 181/99999 100%|██████████| 66/66 [00:00<00:00, 74.38it/s, loss=1.7255 obj=1.7411 lr=0.0100]\n",
      "Epoch 182/99999 100%|██████████| 66/66 [00:01<00:00, 51.21it/s, loss=1.7278 obj=1.7435 lr=0.0100]\n",
      "Epoch 183/99999 100%|██████████| 66/66 [00:00<00:00, 66.38it/s, loss=1.7264 obj=1.7422 lr=0.0100]\n",
      "Epoch 184/99999 100%|██████████| 66/66 [00:01<00:00, 61.45it/s, loss=1.7264 obj=1.7422 lr=0.0100]\n",
      "Epoch 185/99999 100%|██████████| 66/66 [00:01<00:00, 54.77it/s, loss=1.7292 obj=1.7451 lr=0.0100]\n",
      "Epoch 186/99999 100%|██████████| 66/66 [00:01<00:00, 57.38it/s, loss=1.7271 obj=1.7431 lr=0.0100]\n",
      "Epoch 187/99999 100%|██████████| 66/66 [00:00<00:00, 69.94it/s, loss=1.7181 obj=1.7342 lr=0.0100]\n",
      "Epoch 188/99999 100%|██████████| 66/66 [00:00<00:00, 75.42it/s, loss=1.7152 obj=1.7313 lr=0.0100]\n",
      "Epoch 189/99999 100%|██████████| 66/66 [00:00<00:00, 74.18it/s, loss=1.7142 obj=1.7304 lr=0.0100]\n",
      "Epoch 190/99999 100%|██████████| 66/66 [00:00<00:00, 74.73it/s, loss=1.7129 obj=1.7291 lr=0.0100]\n",
      "Epoch 191/99999 100%|██████████| 66/66 [00:01<00:00, 59.74it/s, loss=1.7132 obj=1.7295 lr=0.0100]\n",
      "Epoch 192/99999 100%|██████████| 66/66 [00:00<00:00, 76.65it/s, loss=1.7144 obj=1.7307 lr=0.0100]\n",
      "Epoch 193/99999 100%|██████████| 66/66 [00:00<00:00, 67.77it/s, loss=1.7164 obj=1.7328 lr=0.0100]\n",
      "Epoch 194/99999 100%|██████████| 66/66 [00:00<00:00, 70.38it/s, loss=1.7176 obj=1.7341 lr=0.0100]\n",
      "Epoch 195/99999 100%|██████████| 66/66 [00:00<00:00, 67.31it/s, loss=1.7169 obj=1.7334 lr=0.0100]\n",
      "Epoch 196/99999 100%|██████████| 66/66 [00:00<00:00, 75.51it/s, loss=1.7241 obj=1.7407 lr=0.0100]\n",
      "Epoch 197/99999 100%|██████████| 66/66 [00:00<00:00, 68.77it/s, loss=1.7193 obj=1.7359 lr=0.0100]\n",
      "Epoch 198/99999 100%|██████████| 66/66 [00:00<00:00, 69.96it/s, loss=1.7150 obj=1.7317 lr=0.0100]\n",
      "Epoch 199/99999 100%|██████████| 66/66 [00:00<00:00, 71.54it/s, loss=1.7133 obj=1.7301 lr=0.0100]\n",
      "Epoch 200/99999 100%|██████████| 66/66 [00:00<00:00, 80.22it/s, loss=1.7129 obj=1.7298 lr=0.0100]\n",
      "Epoch 201/99999 100%|██████████| 66/66 [00:01<00:00, 65.83it/s, loss=1.7144 obj=1.7313 lr=0.0100]\n",
      "Epoch 202/99999 100%|██████████| 66/66 [00:00<00:00, 76.42it/s, loss=1.7137 obj=1.7307 lr=0.0100]\n",
      "Epoch 203/99999 100%|██████████| 66/66 [00:00<00:00, 76.22it/s, loss=1.7115 obj=1.7285 lr=0.0100]\n",
      "Epoch 204/99999 100%|██████████| 66/66 [00:00<00:00, 68.27it/s, loss=1.7098 obj=1.7269 lr=0.0100]\n",
      "Epoch 205/99999 100%|██████████| 66/66 [00:00<00:00, 67.77it/s, loss=1.7109 obj=1.7280 lr=0.0100]\n",
      "Epoch 206/99999 100%|██████████| 66/66 [00:00<00:00, 67.96it/s, loss=1.7119 obj=1.7291 lr=0.0100]\n",
      "Epoch 207/99999 100%|██████████| 66/66 [00:00<00:00, 75.49it/s, loss=1.7127 obj=1.7299 lr=0.0100]\n",
      "Epoch 208/99999 100%|██████████| 66/66 [00:00<00:00, 77.64it/s, loss=1.7112 obj=1.7285 lr=0.0100]\n",
      "Epoch 209/99999 100%|██████████| 66/66 [00:00<00:00, 66.43it/s, loss=1.7126 obj=1.7300 lr=0.0100]\n",
      "Epoch 210/99999 100%|██████████| 66/66 [00:00<00:00, 70.38it/s, loss=1.7136 obj=1.7310 lr=0.0100]\n",
      "Epoch 211/99999 100%|██████████| 66/66 [00:00<00:00, 69.52it/s, loss=1.7098 obj=1.7273 lr=0.0100]\n",
      "Epoch 212/99999 100%|██████████| 66/66 [00:00<00:00, 75.57it/s, loss=1.7087 obj=1.7262 lr=0.0100]\n",
      "Epoch 213/99999 100%|██████████| 66/66 [00:00<00:00, 72.02it/s, loss=1.7086 obj=1.7261 lr=0.0100]\n",
      "Epoch 214/99999 100%|██████████| 66/66 [00:00<00:00, 66.95it/s, loss=1.7081 obj=1.7257 lr=0.0100]\n",
      "Epoch 215/99999 100%|██████████| 66/66 [00:01<00:00, 51.99it/s, loss=1.7103 obj=1.7280 lr=0.0100]\n",
      "Epoch 216/99999 100%|██████████| 66/66 [00:01<00:00, 59.42it/s, loss=1.7118 obj=1.7295 lr=0.0100]\n",
      "Epoch 217/99999 100%|██████████| 66/66 [00:01<00:00, 59.28it/s, loss=1.7172 obj=1.7350 lr=0.0100]\n",
      "Epoch 218/99999 100%|██████████| 66/66 [00:01<00:00, 64.06it/s, loss=1.7177 obj=1.7356 lr=0.0100]\n",
      "Epoch 219/99999 100%|██████████| 66/66 [00:01<00:00, 61.08it/s, loss=1.7148 obj=1.7326 lr=0.0100]\n",
      "Epoch 220/99999 100%|██████████| 66/66 [00:00<00:00, 66.07it/s, loss=1.7143 obj=1.7322 lr=0.0100]\n",
      "Epoch 221/99999 100%|██████████| 66/66 [00:00<00:00, 69.48it/s, loss=1.7141 obj=1.7321 lr=0.0100]\n",
      "Epoch 222/99999 100%|██████████| 66/66 [00:00<00:00, 79.54it/s, loss=1.7095 obj=1.7275 lr=0.0100]\n",
      "Epoch 223/99999 100%|██████████| 66/66 [00:00<00:00, 80.20it/s, loss=1.7079 obj=1.7259 lr=0.0100]\n",
      "Epoch 224/99999 100%|██████████| 66/66 [00:00<00:00, 80.77it/s, loss=1.7078 obj=1.7259 lr=0.0100]\n",
      "Epoch 225/99999 100%|██████████| 66/66 [00:00<00:00, 75.82it/s, loss=1.7065 obj=1.7247 lr=0.0100]\n",
      "Epoch 226/99999 100%|██████████| 66/66 [00:00<00:00, 82.45it/s, loss=1.7063 obj=1.7246 lr=0.0100]\n",
      "Epoch 227/99999 100%|██████████| 66/66 [00:00<00:00, 68.50it/s, loss=1.7077 obj=1.7260 lr=0.0100]\n",
      "Epoch 228/99999 100%|██████████| 66/66 [00:01<00:00, 54.64it/s, loss=1.7115 obj=1.7299 lr=0.0100]\n",
      "Epoch 229/99999 100%|██████████| 66/66 [00:00<00:00, 73.35it/s, loss=1.7109 obj=1.7294 lr=0.0100]\n",
      "Epoch 230/99999 100%|██████████| 66/66 [00:00<00:00, 76.89it/s, loss=1.7098 obj=1.7283 lr=0.0100]\n",
      "Epoch 231/99999 100%|██████████| 66/66 [00:00<00:00, 77.89it/s, loss=1.7076 obj=1.7262 lr=0.0100]\n",
      "Epoch 232/99999 100%|██████████| 66/66 [00:00<00:00, 74.13it/s, loss=1.7070 obj=1.7257 lr=0.0100]\n",
      "Epoch 233/99999 100%|██████████| 66/66 [00:00<00:00, 74.04it/s, loss=1.7063 obj=1.7251 lr=0.0100]\n",
      "Epoch 234/99999 100%|██████████| 66/66 [00:00<00:00, 71.38it/s, loss=1.7045 obj=1.7232 lr=0.0100]\n",
      "Epoch 235/99999 100%|██████████| 66/66 [00:01<00:00, 62.94it/s, loss=1.7051 obj=1.7239 lr=0.0100]\n",
      "Epoch 236/99999 100%|██████████| 66/66 [00:01<00:00, 65.20it/s, loss=1.7042 obj=1.7231 lr=0.0100]\n",
      "Epoch 237/99999 100%|██████████| 66/66 [00:00<00:00, 74.16it/s, loss=1.7044 obj=1.7233 lr=0.0100]\n",
      "Epoch 238/99999 100%|██████████| 66/66 [00:00<00:00, 71.29it/s, loss=1.7027 obj=1.7217 lr=0.0100]\n",
      "Epoch 239/99999 100%|██████████| 66/66 [00:01<00:00, 61.23it/s, loss=1.7050 obj=1.7240 lr=0.0100]\n",
      "Epoch 240/99999 100%|██████████| 66/66 [00:00<00:00, 73.86it/s, loss=1.7058 obj=1.7248 lr=0.0100]\n",
      "Epoch 241/99999 100%|██████████| 66/66 [00:00<00:00, 67.24it/s, loss=1.7085 obj=1.7275 lr=0.0100]\n",
      "Epoch 242/99999 100%|██████████| 66/66 [00:00<00:00, 66.47it/s, loss=1.7057 obj=1.7248 lr=0.0100]\n",
      "Epoch 243/99999 100%|██████████| 66/66 [00:01<00:00, 65.49it/s, loss=1.7023 obj=1.7215 lr=0.0100]\n",
      "Epoch 244/99999 100%|██████████| 66/66 [00:01<00:00, 65.95it/s, loss=1.7043 obj=1.7235 lr=0.0100]\n",
      "Epoch 245/99999 100%|██████████| 66/66 [00:00<00:00, 69.70it/s, loss=1.7076 obj=1.7269 lr=0.0100]\n",
      "Epoch 246/99999 100%|██████████| 66/66 [00:00<00:00, 71.24it/s, loss=1.7093 obj=1.7286 lr=0.0100]\n",
      "Epoch 247/99999 100%|██████████| 66/66 [00:00<00:00, 71.93it/s, loss=1.7070 obj=1.7264 lr=0.0100]\n",
      "Epoch 248/99999 100%|██████████| 66/66 [00:01<00:00, 44.52it/s, loss=1.7068 obj=1.7262 lr=0.0100]\n",
      "Epoch 249/99999 100%|██████████| 66/66 [00:01<00:00, 58.36it/s, loss=1.7067 obj=1.7262 lr=0.0100]\n",
      "Epoch 250/99999 100%|██████████| 66/66 [00:00<00:00, 71.57it/s, loss=1.7085 obj=1.7280 lr=0.0100]\n",
      "Epoch 251/99999 100%|██████████| 66/66 [00:00<00:00, 76.39it/s, loss=1.7063 obj=1.7259 lr=0.0100]\n",
      "Epoch 252/99999 100%|██████████| 66/66 [00:00<00:00, 67.90it/s, loss=1.7058 obj=1.7254 lr=0.0100]\n",
      "Epoch 253/99999 100%|██████████| 66/66 [00:00<00:00, 67.52it/s, loss=1.7050 obj=1.7247 lr=0.0100]\n",
      "Epoch 254/99999 100%|██████████| 66/66 [00:00<00:00, 72.93it/s, loss=1.7092 obj=1.7290 lr=0.0100]\n",
      "Epoch 255/99999 100%|██████████| 66/66 [00:00<00:00, 73.22it/s, loss=1.7088 obj=1.7286 lr=0.0100]\n",
      "Epoch 256/99999 100%|██████████| 66/66 [00:00<00:00, 71.83it/s, loss=1.7081 obj=1.7279 lr=0.0100]\n",
      "Epoch 257/99999 100%|██████████| 66/66 [00:00<00:00, 78.24it/s, loss=1.7112 obj=1.7311 lr=0.0100]\n",
      "Epoch 258/99999 100%|██████████| 66/66 [00:00<00:00, 76.08it/s, loss=1.7165 obj=1.7365 lr=0.0100]\n",
      "Epoch 259/99999 100%|██████████| 66/66 [00:00<00:00, 80.78it/s, loss=1.7208 obj=1.7408 lr=0.0100]\n",
      "Epoch 260/99999 100%|██████████| 66/66 [00:01<00:00, 63.54it/s, loss=1.7187 obj=1.7388 lr=0.0100]\n",
      "Epoch 261/99999 100%|██████████| 66/66 [00:01<00:00, 62.78it/s, loss=1.7194 obj=1.7396 lr=0.0100]\n",
      "Epoch 262/99999 100%|██████████| 66/66 [00:01<00:00, 60.44it/s, loss=1.7115 obj=1.7317 lr=0.0100]\n",
      "Epoch 263/99999 100%|██████████| 66/66 [00:01<00:00, 51.11it/s, loss=1.7008 obj=1.7210 lr=0.0100]\n",
      "Epoch 264/99999 100%|██████████| 66/66 [00:00<00:00, 68.54it/s, loss=1.6974 obj=1.7177 lr=0.0100]\n",
      "Epoch 265/99999 100%|██████████| 66/66 [00:01<00:00, 63.77it/s, loss=1.6964 obj=1.7167 lr=0.0100]\n",
      "Epoch 266/99999 100%|██████████| 66/66 [00:00<00:00, 66.68it/s, loss=1.6964 obj=1.7167 lr=0.0100]\n",
      "Epoch 267/99999 100%|██████████| 66/66 [00:00<00:00, 68.35it/s, loss=1.6977 obj=1.7181 lr=0.0100]\n",
      "Epoch 268/99999 100%|██████████| 66/66 [00:00<00:00, 67.44it/s, loss=1.6986 obj=1.7191 lr=0.0100]\n",
      "Epoch 269/99999 100%|██████████| 66/66 [00:01<00:00, 65.30it/s, loss=1.7003 obj=1.7208 lr=0.0100]\n",
      "Epoch 270/99999 100%|██████████| 66/66 [00:00<00:00, 73.12it/s, loss=1.7000 obj=1.7205 lr=0.0100]\n",
      "Epoch 271/99999 100%|██████████| 66/66 [00:00<00:00, 77.94it/s, loss=1.6994 obj=1.7200 lr=0.0100]\n",
      "Epoch 272/99999 100%|██████████| 66/66 [00:00<00:00, 76.31it/s, loss=1.6997 obj=1.7204 lr=0.0100]\n",
      "Epoch 273/99999 100%|██████████| 66/66 [00:00<00:00, 69.44it/s, loss=1.7003 obj=1.7210 lr=0.0100]\n",
      "Epoch 274/99999 100%|██████████| 66/66 [00:01<00:00, 58.13it/s, loss=1.6989 obj=1.7197 lr=0.0100]\n",
      "Epoch 275/99999 100%|██████████| 66/66 [00:00<00:00, 66.16it/s, loss=1.6987 obj=1.7195 lr=0.0100]\n",
      "Epoch 276/99999 100%|██████████| 66/66 [00:00<00:00, 74.83it/s, loss=1.7016 obj=1.7225 lr=0.0100]\n",
      "Epoch 277/99999 100%|██████████| 66/66 [00:00<00:00, 77.74it/s, loss=1.7010 obj=1.7219 lr=0.0100]\n",
      "Epoch 278/99999 100%|██████████| 66/66 [00:00<00:00, 66.91it/s, loss=1.7072 obj=1.7281 lr=0.0100]\n",
      "Epoch 279/99999 100%|██████████| 66/66 [00:00<00:00, 77.54it/s, loss=1.7141 obj=1.7351 lr=0.0100]\n",
      "Epoch 280/99999 100%|██████████| 66/66 [00:00<00:00, 73.16it/s, loss=1.7147 obj=1.7357 lr=0.0100]\n",
      "Epoch 281/99999 100%|██████████| 66/66 [00:00<00:00, 75.12it/s, loss=1.7081 obj=1.7292 lr=0.0100]\n",
      "Epoch 282/99999 100%|██████████| 66/66 [00:00<00:00, 75.09it/s, loss=1.7001 obj=1.7213 lr=0.0100]\n",
      "Epoch 283/99999 100%|██████████| 66/66 [00:01<00:00, 64.37it/s, loss=1.6948 obj=1.7160 lr=0.0100]\n",
      "Epoch 284/99999 100%|██████████| 66/66 [00:01<00:00, 65.48it/s, loss=1.6937 obj=1.7149 lr=0.0100]\n",
      "Epoch 285/99999 100%|██████████| 66/66 [00:01<00:00, 57.84it/s, loss=1.6947 obj=1.7159 lr=0.0100]\n",
      "Epoch 286/99999 100%|██████████| 66/66 [00:01<00:00, 62.54it/s, loss=1.6948 obj=1.7160 lr=0.0100]\n",
      "Epoch 287/99999 100%|██████████| 66/66 [00:00<00:00, 67.74it/s, loss=1.6949 obj=1.7162 lr=0.0100]\n",
      "Epoch 288/99999 100%|██████████| 66/66 [00:01<00:00, 63.67it/s, loss=1.6944 obj=1.7157 lr=0.0100]\n",
      "Epoch 289/99999 100%|██████████| 66/66 [00:00<00:00, 70.14it/s, loss=1.6944 obj=1.7158 lr=0.0100]\n",
      "Epoch 290/99999 100%|██████████| 66/66 [00:01<00:00, 65.00it/s, loss=1.6954 obj=1.7168 lr=0.0100]\n",
      "Epoch 291/99999 100%|██████████| 66/66 [00:01<00:00, 62.06it/s, loss=1.6957 obj=1.7172 lr=0.0100]\n",
      "Epoch 292/99999 100%|██████████| 66/66 [00:01<00:00, 60.84it/s, loss=1.6987 obj=1.7202 lr=0.0100]\n",
      "Epoch 293/99999 100%|██████████| 66/66 [00:00<00:00, 71.57it/s, loss=1.6988 obj=1.7204 lr=0.0100]\n",
      "Epoch 294/99999   0%|          | 0/66 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mbatches_per_epoch, bar_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{l_bar}\u001b[39;00m\u001b[38;5;132;01m{bar:10}\u001b[39;00m\u001b[38;5;132;01m{r_bar}\u001b[39;00m\u001b[38;5;132;01m{bar:-10b}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 60\u001b[0m         \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         batch_i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m batch_i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m batch_i \u001b[38;5;241m==\u001b[39m batches_per_epoch:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ts = int(time())\n",
    "epochs = 99999\n",
    "#optimizer = SGD(learning_rate=0.03, nesterov=True, momentum=0.9, clipnorm=1)\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=100 * 10_000,\n",
    "    decay_rate=0.98,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "obj_tracker = keras.metrics.Mean(name=\"obj\")\n",
    "callbacks = CallbackList([\n",
    "    ModelCheckpoint(f\"checkpoints/{ts}\" + \"/model-{epoch:04d}-{loss:.3f}.keras\", monitor=\"loss\", save_best_only=True),\n",
    "    TensorBoard(log_dir=f\"./logs/{ts}\", write_graph=False)    \n",
    "], model=chess_model)\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch):\n",
    "    batch = tf.reshape(tf.io.decode_raw(batch, tf.int64), (-1, 3, 12))\n",
    "\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Run the forward pass of the layer.\n",
    "        # The operations that the layer applies\n",
    "        # to its inputs are going to be recorded\n",
    "        # on the GradientTape.\n",
    "        logits = tf.reshape(chess_model(tf.reshape(batch, (-1, 12)), training=True), (-1, 3))  # Logits for this minibatch\n",
    "\n",
    "        # Compute the loss value for this minibatch.\n",
    "        loss_value, obj_value = custom_loss(logits)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss.\n",
    "    grads = tape.gradient(obj_value, chess_model.trainable_weights)\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients(zip(grads, chess_model.trainable_weights))\n",
    "\n",
    "    # Update metrics\n",
    "    loss_tracker.update_state(loss_value)\n",
    "    obj_tracker.update_state(obj_value)\n",
    "\n",
    "callbacks.on_train_begin()\n",
    "for epoch in range(epochs):\n",
    "    loss_tracker.reset_states()\n",
    "    obj_tracker.reset_states()\n",
    "    callbacks.on_epoch_begin(epoch)\n",
    "\n",
    "    if math.isnan(loss_tracker.result()):\n",
    "        print(\"Loss is NaN, exiting\")\n",
    "        break\n",
    "\n",
    "    batch_i = 0\n",
    "    batch_i_last = 0\n",
    "    with tqdm(total=batches_per_epoch, bar_format=f\"Epoch {epoch+1}/{epochs}\" + \" {l_bar}{bar:10}{r_bar}{bar:-10b}\") as pbar:\n",
    "        for batch in dataset:\n",
    "            train_step(batch)\n",
    "\n",
    "            batch_i += 1\n",
    "            if batch_i % 10 == 0 or batch_i == batches_per_epoch:\n",
    "                pbar.set_postfix_str(f\"loss={loss_tracker.result():.4f} obj={obj_tracker.result():.4f} lr={optimizer.learning_rate.numpy():.4f}\")\n",
    "                pbar.update(batch_i - batch_i_last)\n",
    "                batch_i_last = batch_i\n",
    "\n",
    "    mean_tw = sum([tf.reduce_mean(tf.abs(tw)).numpy() for tw in chess_model.trainable_weights])\n",
    "\n",
    "    logs = {\n",
    "        \"loss\": loss_tracker.result(),\n",
    "        \"obj\": obj_tracker.result(),\n",
    "        \"lr\": optimizer.learning_rate.numpy(),\n",
    "        \"mean_tw\": mean_tw\n",
    "    }\n",
    "\n",
    "    # NOTE: GOOD/BAD based on the POV of WHO JUST PLAYED\n",
    "    samples = [\n",
    "        # 2kr3r/1pp1pp1p/1p6/q4bP1/2B5/4BP2/Pb1NQK1P/R6R w - - 0 18\n",
    "        #(\"2r4r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\", # good\n",
    "        #\"3r3r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\"), # bad\n",
    "\n",
    "        # mega blunder\n",
    "        # 2n1kb1r/r2n1ppp/1R2p3/p2pP1N1/Q2P3q/1N2B3/P4PPP/1R4K1 w k - 1 18\n",
    "        (\"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4r3/R2N1PPP/2N1KB1R w K - 0 18\", # good\n",
    "        \"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4P3/R2N1PPP/1rN1KB1R w K - 2 18\"), # bad\n",
    "\n",
    "        # inaccuracy\n",
    "        # r2qkb1r/1pp1nppp/p1n1p3/1B1pPb2/3P4/5N2/PPP2PPP/RNBQ1RK1 w kq - 0 7\n",
    "        (\"rnbq1rk1/ppp2ppp/5n2/3p4/3PpB2/P1b1P3/1PP1NPPP/R2QKB1R w KQ - 0 7\", # good\n",
    "        \"rnbq1rk1/ppp1bppp/5n2/3p4/3PpB2/P1N1P3/1PP1NPPP/R2QKB1R w KQ - 1 7\") # bad\n",
    "    ]\n",
    "\n",
    "    import chess\n",
    "    for (i, (good_fen, bad_fen)) in enumerate(samples):\n",
    "        good_board = chess.Board(good_fen)\n",
    "        bad_board = chess.Board(bad_fen)\n",
    "        pred = chess_model.predict(tf.concat([encode_board(good_board), encode_board(bad_board)], axis=0), verbose=0)\n",
    "        \n",
    "        logs[f\"good{i}\"] = pred[0][0]\n",
    "        logs[f\"bad{i}\"] = pred[1][0]\n",
    "\n",
    "    callbacks.on_epoch_end(epoch, logs)\n",
    "callbacks.on_train_end()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "b = chess.Board(\"r2qkb1r/1pp1nppp/p1n1p3/3pPb2/3P4/5N2/PPP1BPPP/RNBQ1RK1 b kq - 1 7\").mirror()\n",
    "print(b.fen())\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_model = keras.models.load_model(\"/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/models/model-0682-1.751.keras\", compile=False)\n",
    "\n",
    "# NOTE: GOOD/BAD based on the POV of WHO JUST PLAYED\n",
    "samples = [\n",
    "    # 2kr3r/1pp1pp1p/1p6/q4bP1/2B5/4BP2/Pb1NQK1P/R6R w - - 0 18\n",
    "    #(\"2r4r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\", # good\n",
    "    #\"3r3r/pB1nqk1p/4bp2/2b5/Q4Bp1/1P6/1PP1PP1P/2KR3R w - - 1 18\"), # bad\n",
    "\n",
    "    # mega blunder\n",
    "    # 2n1kb1r/r2n1ppp/1R2p3/p2pP1N1/Q2P3q/1N2B3/P4PPP/1R4K1 w k - 1 18\n",
    "    (\"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4r3/R2N1PPP/2N1KB1R w K - 0 18\", # good\n",
    "    \"1r4k1/p4ppp/1n2b3/q2p3Q/P2Pp1n1/4P3/R2N1PPP/1rN1KB1R w K - 2 18\"), # bad\n",
    "\n",
    "    # inaccuracy\n",
    "    # r2qkb1r/1pp1nppp/p1n1p3/1B1pPb2/3P4/5N2/PPP2PPP/RNBQ1RK1 w kq - 0 7\n",
    "    (\"rnbq1rk1/ppp2ppp/5n2/3p4/3PpB2/P1b1P3/1PP1NPPP/R2QKB1R w KQ - 0 7\", # good\n",
    "    \"rnbq1rk1/ppp1bppp/5n2/3p4/3PpB2/P1N1P3/1PP1NPPP/R2QKB1R w KQ - 1 7\") # bad\n",
    "]\n",
    "\n",
    "import chess\n",
    "for (i, (good_fen, bad_fen)) in enumerate(samples):\n",
    "    good_board = chess.Board(good_fen)\n",
    "    bad_board = chess.Board(bad_fen)\n",
    "    pred = chess_model.predict(tf.concat([encode_board(good_board), encode_board(bad_board)], axis=0), verbose=0)\n",
    "    \n",
    "    print(pred[0][0] - pred[1][0]) # good - bad should be +\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
