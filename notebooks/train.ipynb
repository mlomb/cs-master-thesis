{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from lib.service import SamplesService\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_int64_bitset(x):\n",
    "    \"\"\"\n",
    "    Convert a 64-bit integer into a 64-element float tensor\n",
    "    \"\"\"\n",
    "    masks = 2 ** torch.arange(64, dtype=torch.int64, device='cuda')\n",
    "    expanded = torch.bitwise_and(x.unsqueeze(-1), masks).ne(0).to(torch.float32)\n",
    "    return expanded\n",
    "\n",
    "class ChessModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features):\n",
    "        super(ChessModel, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear1 = torch.nn.Linear(num_features, 1024)\n",
    "        self.linear2 = torch.nn.Linear(1024, 64)\n",
    "        self.linear3 = torch.nn.Linear(64, 64)\n",
    "        self.linear4 = torch.nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PQRLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PQRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred):\n",
    "        pred = pred.reshape(-1, 3)\n",
    "        \n",
    "        p = pred[:,0]\n",
    "        q = pred[:,1]\n",
    "        r = pred[:,2]\n",
    "        \n",
    "        a = -torch.mean(torch.log(torch.sigmoid(r - q)))\n",
    "        b = torch.mean(torch.square(p + q))\n",
    "\n",
    "        loss = a + b\n",
    "\n",
    "        return loss\n",
    "\n",
    "class EvalLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EvalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, model, target):\n",
    "        scaling = 356.0\n",
    "\n",
    "        # scale CP score to engine units [-10_000, 10_000]\n",
    "        target = target * scaling / 100.0\n",
    "\n",
    "        # targets are in CP-space change it to WDL-space [0, 1]\n",
    "        wdl_model = torch.sigmoid(model / scaling)\n",
    "        wdl_target = torch.sigmoid(target / scaling)\n",
    "\n",
    "        loss = torch.pow(torch.abs(wdl_model - wdl_target), 2.5)\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 17:20:05.516094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-07 17:20:05.516303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-07 17:20:05.542951: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-07 17:20:05.629231: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 17:20:08.233802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Epoch 0: 100%|██████████| 1000/1000 [00:12<00:00, 81.52it/s]\n",
      "Epoch 1: 100%|██████████| 1000/1000 [00:10<00:00, 95.39it/s]\n",
      "Epoch 2: 100%|██████████| 1000/1000 [00:13<00:00, 73.57it/s]\n",
      "Epoch 3: 100%|██████████| 1000/1000 [00:11<00:00, 88.84it/s]\n",
      "Epoch 4: 100%|██████████| 1000/1000 [00:11<00:00, 89.81it/s]\n",
      "Epoch 5: 100%|██████████| 1000/1000 [00:10<00:00, 93.80it/s]\n",
      "Epoch 6: 100%|██████████| 1000/1000 [00:09<00:00, 105.12it/s]\n",
      "Epoch 7: 100%|██████████| 1000/1000 [00:10<00:00, 91.14it/s]\n",
      "Epoch 8: 100%|██████████| 1000/1000 [00:09<00:00, 105.39it/s]\n",
      "Epoch 9: 100%|██████████| 1000/1000 [00:09<00:00, 100.45it/s]\n",
      "Epoch 10: 100%|██████████| 1000/1000 [00:08<00:00, 111.78it/s]\n",
      "Epoch 11: 100%|██████████| 1000/1000 [00:08<00:00, 114.15it/s]\n",
      "Epoch 12: 100%|██████████| 1000/1000 [00:08<00:00, 117.87it/s]\n",
      "Epoch 13: 100%|██████████| 1000/1000 [00:08<00:00, 115.49it/s]\n",
      "Epoch 14: 100%|██████████| 1000/1000 [00:07<00:00, 125.60it/s]\n",
      "Epoch 15: 100%|██████████| 1000/1000 [00:08<00:00, 117.41it/s]\n",
      "Epoch 16: 100%|██████████| 1000/1000 [00:09<00:00, 106.68it/s]\n",
      "Epoch 17: 100%|██████████| 1000/1000 [00:10<00:00, 97.86it/s]\n",
      "Epoch 18: 100%|██████████| 1000/1000 [00:09<00:00, 106.91it/s]\n",
      "Epoch 19: 100%|██████████| 1000/1000 [00:08<00:00, 121.68it/s]\n",
      "Epoch 20: 100%|██████████| 1000/1000 [00:08<00:00, 117.89it/s]\n",
      "Epoch 21: 100%|██████████| 1000/1000 [00:10<00:00, 99.94it/s]\n",
      "Epoch 22: 100%|██████████| 1000/1000 [00:09<00:00, 104.51it/s]\n",
      "Epoch 23: 100%|██████████| 1000/1000 [00:11<00:00, 89.15it/s]\n",
      "Epoch 24: 100%|██████████| 1000/1000 [00:09<00:00, 103.91it/s]\n",
      "Epoch 25: 100%|██████████| 1000/1000 [00:10<00:00, 92.15it/s]\n",
      "Epoch 26: 100%|██████████| 1000/1000 [00:08<00:00, 117.21it/s]\n",
      "Epoch 27: 100%|██████████| 1000/1000 [00:08<00:00, 117.72it/s]\n",
      "Epoch 28: 100%|██████████| 1000/1000 [00:08<00:00, 121.29it/s]\n",
      "Epoch 29: 100%|██████████| 1000/1000 [00:10<00:00, 99.00it/s]\n",
      "Epoch 30: 100%|██████████| 1000/1000 [00:09<00:00, 106.31it/s]\n",
      "Epoch 31: 100%|██████████| 1000/1000 [00:09<00:00, 109.58it/s]\n",
      "Epoch 32: 100%|██████████| 1000/1000 [00:08<00:00, 111.65it/s]\n",
      "Epoch 33: 100%|██████████| 1000/1000 [00:08<00:00, 118.23it/s]\n",
      "Epoch 34: 100%|██████████| 1000/1000 [00:09<00:00, 105.61it/s]\n",
      "Epoch 35: 100%|██████████| 1000/1000 [00:09<00:00, 104.60it/s]\n",
      "Epoch 36: 100%|██████████| 1000/1000 [00:08<00:00, 115.03it/s]\n",
      "Epoch 37: 100%|██████████| 1000/1000 [00:09<00:00, 109.16it/s]\n",
      "Epoch 38: 100%|██████████| 1000/1000 [00:09<00:00, 104.33it/s]\n",
      "Epoch 39: 100%|██████████| 1000/1000 [00:08<00:00, 112.57it/s]\n",
      "Epoch 40: 100%|██████████| 1000/1000 [00:08<00:00, 117.16it/s]\n",
      "Epoch 41: 100%|██████████| 1000/1000 [00:08<00:00, 118.37it/s]\n",
      "Epoch 42: 100%|██████████| 1000/1000 [00:08<00:00, 117.39it/s]\n",
      "Epoch 43: 100%|██████████| 1000/1000 [00:08<00:00, 113.89it/s]\n",
      "Epoch 44: 100%|██████████| 1000/1000 [00:09<00:00, 104.92it/s]\n",
      "Epoch 45: 100%|██████████| 1000/1000 [00:09<00:00, 102.24it/s]\n",
      "Epoch 46: 100%|██████████| 1000/1000 [00:09<00:00, 105.89it/s]\n",
      "Epoch 47: 100%|██████████| 1000/1000 [00:09<00:00, 109.01it/s]\n",
      "Epoch 48: 100%|██████████| 1000/1000 [00:08<00:00, 115.15it/s]\n",
      "Epoch 49: 100%|██████████| 1000/1000 [00:09<00:00, 100.69it/s]\n",
      "Epoch 50: 100%|██████████| 1000/1000 [00:09<00:00, 104.12it/s]\n",
      "Epoch 51: 100%|██████████| 1000/1000 [00:09<00:00, 104.11it/s]\n",
      "Epoch 52: 100%|██████████| 1000/1000 [00:09<00:00, 101.90it/s]\n",
      "Epoch 53: 100%|██████████| 1000/1000 [00:08<00:00, 117.71it/s]\n",
      "Epoch 54: 100%|██████████| 1000/1000 [00:09<00:00, 102.52it/s]\n",
      "Epoch 55: 100%|██████████| 1000/1000 [00:09<00:00, 105.22it/s]\n",
      "Epoch 56: 100%|██████████| 1000/1000 [00:10<00:00, 98.31it/s]\n",
      "Epoch 57: 100%|██████████| 1000/1000 [00:09<00:00, 105.74it/s]\n",
      "Epoch 58: 100%|██████████| 1000/1000 [00:09<00:00, 102.58it/s]\n",
      "Epoch 59: 100%|██████████| 1000/1000 [00:10<00:00, 93.48it/s]\n",
      "Epoch 60: 100%|██████████| 1000/1000 [00:09<00:00, 107.78it/s]\n",
      "Epoch 61: 100%|██████████| 1000/1000 [00:08<00:00, 117.97it/s]\n",
      "Epoch 62: 100%|██████████| 1000/1000 [00:09<00:00, 108.16it/s]\n",
      "Epoch 63: 100%|██████████| 1000/1000 [00:09<00:00, 104.71it/s]\n",
      "Epoch 64: 100%|██████████| 1000/1000 [00:10<00:00, 97.28it/s] \n",
      "Epoch 65: 100%|██████████| 1000/1000 [00:10<00:00, 92.16it/s]\n",
      "Epoch 66: 100%|██████████| 1000/1000 [00:08<00:00, 111.99it/s]\n",
      "Epoch 67: 100%|██████████| 1000/1000 [00:08<00:00, 120.82it/s]\n",
      "Epoch 68: 100%|██████████| 1000/1000 [00:08<00:00, 122.14it/s]\n",
      "Epoch 69: 100%|██████████| 1000/1000 [00:08<00:00, 122.13it/s]\n",
      "Epoch 70: 100%|██████████| 1000/1000 [00:08<00:00, 122.62it/s]\n",
      "Epoch 71: 100%|██████████| 1000/1000 [00:08<00:00, 121.67it/s]\n",
      "Epoch 72: 100%|██████████| 1000/1000 [00:08<00:00, 121.53it/s]\n",
      "Epoch 73: 100%|██████████| 1000/1000 [00:08<00:00, 120.66it/s]\n",
      "Epoch 74: 100%|██████████| 1000/1000 [00:08<00:00, 117.30it/s]\n",
      "Epoch 75: 100%|██████████| 1000/1000 [00:08<00:00, 118.80it/s]\n",
      "Epoch 76: 100%|██████████| 1000/1000 [00:08<00:00, 120.32it/s]\n",
      "Epoch 77: 100%|██████████| 1000/1000 [00:08<00:00, 120.56it/s]\n",
      "Epoch 78: 100%|██████████| 1000/1000 [00:08<00:00, 120.82it/s]\n",
      "Epoch 79: 100%|██████████| 1000/1000 [00:08<00:00, 118.61it/s]\n",
      "Epoch 80: 100%|██████████| 1000/1000 [00:08<00:00, 111.11it/s]\n",
      "Epoch 81: 100%|██████████| 1000/1000 [00:08<00:00, 115.68it/s]\n",
      "Epoch 82: 100%|██████████| 1000/1000 [00:09<00:00, 105.53it/s]\n",
      "Epoch 83: 100%|██████████| 1000/1000 [00:09<00:00, 101.43it/s]\n",
      "Epoch 84: 100%|██████████| 1000/1000 [00:10<00:00, 96.90it/s]\n",
      "Epoch 85: 100%|██████████| 1000/1000 [00:08<00:00, 119.64it/s]\n",
      "Epoch 86: 100%|██████████| 1000/1000 [00:08<00:00, 121.80it/s]\n",
      "Epoch 87: 100%|██████████| 1000/1000 [00:08<00:00, 119.00it/s]\n",
      "Epoch 88: 100%|██████████| 1000/1000 [00:08<00:00, 117.59it/s]\n",
      "Epoch 89: 100%|██████████| 1000/1000 [00:08<00:00, 118.81it/s]\n",
      "Epoch 90: 100%|██████████| 1000/1000 [00:08<00:00, 121.86it/s]\n",
      "Epoch 91: 100%|██████████| 1000/1000 [00:08<00:00, 119.29it/s]\n",
      "Epoch 92: 100%|██████████| 1000/1000 [00:08<00:00, 116.32it/s]\n",
      "Epoch 93: 100%|██████████| 1000/1000 [00:09<00:00, 108.80it/s]\n",
      "Epoch 94: 100%|██████████| 1000/1000 [00:09<00:00, 107.88it/s]\n",
      "Epoch 95: 100%|██████████| 1000/1000 [00:08<00:00, 113.47it/s]\n",
      "Epoch 96: 100%|██████████| 1000/1000 [00:09<00:00, 100.59it/s]\n",
      "Epoch 97: 100%|██████████| 1000/1000 [00:08<00:00, 111.43it/s]\n",
      "Epoch 98: 100%|██████████| 1000/1000 [00:08<00:00, 117.57it/s]\n",
      "Epoch 99: 100%|██████████| 1000/1000 [00:08<00:00, 119.73it/s]\n",
      "Epoch 100: 100%|██████████| 1000/1000 [00:08<00:00, 114.40it/s]\n",
      "Epoch 101: 100%|██████████| 1000/1000 [00:09<00:00, 100.78it/s]\n",
      "Epoch 102: 100%|██████████| 1000/1000 [00:10<00:00, 98.42it/s]\n",
      "Epoch 103: 100%|██████████| 1000/1000 [00:10<00:00, 98.25it/s]\n",
      "Epoch 104: 100%|██████████| 1000/1000 [00:09<00:00, 103.83it/s]\n",
      "Epoch 105: 100%|██████████| 1000/1000 [00:10<00:00, 95.79it/s]\n",
      "Epoch 106: 100%|██████████| 1000/1000 [00:10<00:00, 95.02it/s]\n",
      "Epoch 107: 100%|██████████| 1000/1000 [00:10<00:00, 98.67it/s]\n",
      "Epoch 108: 100%|██████████| 1000/1000 [00:08<00:00, 114.72it/s]\n",
      "Epoch 109: 100%|██████████| 1000/1000 [00:10<00:00, 91.46it/s]\n",
      "Epoch 110: 100%|██████████| 1000/1000 [00:10<00:00, 98.99it/s]\n",
      "Epoch 111: 100%|██████████| 1000/1000 [00:09<00:00, 100.63it/s]\n",
      "Epoch 112: 100%|██████████| 1000/1000 [00:09<00:00, 104.64it/s]\n",
      "Epoch 113: 100%|██████████| 1000/1000 [00:09<00:00, 105.01it/s]\n",
      "Epoch 114: 100%|██████████| 1000/1000 [00:09<00:00, 109.53it/s]\n",
      "Epoch 115: 100%|██████████| 1000/1000 [00:09<00:00, 110.61it/s]\n",
      "Epoch 116: 100%|██████████| 1000/1000 [00:08<00:00, 111.51it/s]\n",
      "Epoch 117: 100%|██████████| 1000/1000 [00:08<00:00, 117.53it/s]\n",
      "Epoch 118: 100%|██████████| 1000/1000 [00:10<00:00, 95.34it/s]\n",
      "Epoch 119: 100%|██████████| 1000/1000 [00:09<00:00, 100.54it/s]\n",
      "Epoch 120: 100%|██████████| 1000/1000 [00:10<00:00, 98.12it/s]\n",
      "Epoch 121: 100%|██████████| 1000/1000 [00:09<00:00, 106.40it/s]\n",
      "Epoch 122: 100%|██████████| 1000/1000 [00:08<00:00, 118.04it/s]\n",
      "Epoch 123: 100%|██████████| 1000/1000 [00:08<00:00, 113.15it/s]\n",
      "Epoch 124: 100%|██████████| 1000/1000 [00:09<00:00, 107.21it/s]\n",
      "Epoch 125: 100%|██████████| 1000/1000 [00:09<00:00, 102.26it/s]\n",
      "Epoch 126: 100%|██████████| 1000/1000 [00:08<00:00, 114.11it/s]\n",
      "Epoch 127: 100%|██████████| 1000/1000 [00:08<00:00, 115.60it/s]\n",
      "Epoch 128: 100%|██████████| 1000/1000 [00:08<00:00, 112.36it/s]\n",
      "Epoch 129: 100%|██████████| 1000/1000 [00:09<00:00, 102.98it/s]\n",
      "Epoch 130: 100%|██████████| 1000/1000 [00:08<00:00, 118.25it/s]\n",
      "Epoch 131: 100%|██████████| 1000/1000 [00:08<00:00, 116.91it/s]\n",
      "Epoch 132: 100%|██████████| 1000/1000 [00:08<00:00, 112.18it/s]\n",
      "Epoch 133: 100%|██████████| 1000/1000 [00:08<00:00, 116.88it/s]\n",
      "Epoch 134: 100%|██████████| 1000/1000 [00:08<00:00, 120.28it/s]\n",
      "Epoch 135: 100%|██████████| 1000/1000 [00:08<00:00, 120.01it/s]\n",
      "Epoch 136: 100%|██████████| 1000/1000 [00:08<00:00, 120.52it/s]\n",
      "Epoch 137: 100%|██████████| 1000/1000 [00:08<00:00, 116.99it/s]\n",
      "Epoch 138: 100%|██████████| 1000/1000 [00:09<00:00, 110.30it/s]\n",
      "Epoch 139: 100%|██████████| 1000/1000 [00:09<00:00, 104.51it/s]\n",
      "Epoch 140: 100%|██████████| 1000/1000 [00:08<00:00, 117.77it/s]\n",
      "Epoch 141: 100%|██████████| 1000/1000 [00:10<00:00, 96.53it/s]\n",
      "Epoch 142: 100%|██████████| 1000/1000 [00:11<00:00, 88.25it/s]\n",
      "Epoch 143: 100%|██████████| 1000/1000 [00:09<00:00, 103.66it/s]\n",
      "Epoch 144: 100%|██████████| 1000/1000 [00:09<00:00, 110.69it/s]\n",
      "Epoch 145: 100%|██████████| 1000/1000 [00:09<00:00, 109.64it/s]\n",
      "Epoch 146: 100%|██████████| 1000/1000 [00:08<00:00, 113.31it/s]\n",
      "Epoch 147: 100%|██████████| 1000/1000 [00:10<00:00, 99.36it/s]\n",
      "Epoch 148: 100%|██████████| 1000/1000 [00:09<00:00, 104.67it/s]\n",
      "Epoch 149: 100%|██████████| 1000/1000 [00:09<00:00, 103.50it/s]\n",
      "Epoch 150: 100%|██████████| 1000/1000 [00:09<00:00, 102.22it/s]\n",
      "Epoch 151: 100%|██████████| 1000/1000 [00:09<00:00, 105.31it/s]\n",
      "Epoch 152: 100%|██████████| 1000/1000 [00:09<00:00, 110.08it/s]\n",
      "Epoch 153: 100%|██████████| 1000/1000 [00:09<00:00, 104.70it/s]\n",
      "Epoch 154: 100%|██████████| 1000/1000 [00:10<00:00, 94.84it/s]\n",
      "Epoch 155: 100%|██████████| 1000/1000 [00:09<00:00, 101.98it/s]\n",
      "Epoch 156: 100%|██████████| 1000/1000 [00:10<00:00, 94.90it/s]\n",
      "Epoch 157: 100%|██████████| 1000/1000 [00:10<00:00, 98.68it/s]\n",
      "Epoch 158: 100%|██████████| 1000/1000 [00:09<00:00, 109.94it/s]\n",
      "Epoch 159: 100%|██████████| 1000/1000 [00:09<00:00, 107.76it/s]\n",
      "Epoch 160: 100%|██████████| 1000/1000 [00:17<00:00, 56.44it/s]\n",
      "Epoch 161: 100%|██████████| 1000/1000 [00:20<00:00, 49.11it/s]\n",
      "Epoch 162: 100%|██████████| 1000/1000 [00:13<00:00, 76.15it/s]\n",
      "Epoch 163: 100%|██████████| 1000/1000 [00:09<00:00, 101.23it/s]\n",
      "Epoch 164: 100%|██████████| 1000/1000 [00:11<00:00, 86.53it/s]\n",
      "Epoch 165: 100%|██████████| 1000/1000 [00:16<00:00, 61.67it/s]\n",
      "Epoch 166:  70%|███████   | 700/1000 [00:15<00:05, 53.40it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "EPOCHS = 100000\n",
    "BATCHES_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "FEATURE_SET = \"basic\"\n",
    "NUM_FEATURES = 768\n",
    "METHOD = \"eval\"\n",
    "\n",
    "if METHOD == \"pqr\":\n",
    "    X_SHAPE = (BATCH_SIZE, 3, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 0)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/pqr-1700/*.csv\")\n",
    "    loss_fn = PQRLoss()\n",
    "elif METHOD == \"eval\":\n",
    "    X_SHAPE = (BATCH_SIZE, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 1)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/eval/*.csv\")\n",
    "    loss_fn = EvalLoss()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "folder = f'runs/{timestamp}_{METHOD}_{FEATURE_SET}_{BATCH_SIZE}'\n",
    "os.makedirs(f'{folder}/models', exist_ok=True)\n",
    "\n",
    "samples_service = SamplesService(x_shape=X_SHAPE, y_shape=Y_SHAPE, inputs=INPUTS, feature_set=FEATURE_SET, method=METHOD)\n",
    "chessmodel = ChessModel(num_features=NUM_FEATURES)\n",
    "chessmodel.cuda()\n",
    "\n",
    "#for i in tqdm(range(1000000)):\n",
    "#    a = samples_service.next_batch()\n",
    "\n",
    "optimizer = torch.optim.Adam(chessmodel.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.001, factor=0.7, patience=100)\n",
    "writer = SummaryWriter(folder)\n",
    "\n",
    "# @torch.compile # 30% speedup\n",
    "def train_step(X, y):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = chessmodel(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Make sure gradient tracking is on\n",
    "chessmodel.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _ in tqdm(range(BATCHES_PER_EPOCH), desc=f'Epoch {epoch}'):\n",
    "        X, y = samples_service.next_batch()\n",
    "    \n",
    "        # expand bitset\n",
    "        X = decode_int64_bitset(X)\n",
    "        X = X.reshape(-1, NUM_FEATURES)\n",
    "\n",
    "        loss = train_step(X, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        if math.isnan(avg_loss):\n",
    "            raise Exception(\"Loss is NaN, exiting\")\n",
    "\n",
    "    avg_loss /= BATCHES_PER_EPOCH\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    writer.add_scalar('Train/loss', avg_loss, epoch)\n",
    "    writer.add_scalar('Train/lr', scheduler._last_lr[0], epoch) # get_last_lr()\n",
    "    writer.add_scalar('Params/mean-l1', torch.mean(chessmodel.linear1.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l2', torch.mean(chessmodel.linear2.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l3', torch.mean(chessmodel.linear3.weight), epoch)\n",
    "    writer.add_scalar('Params/mean-l4', torch.mean(chessmodel.linear4.weight), epoch)\n",
    "    for name, param in chessmodel.named_parameters():\n",
    "        writer.add_histogram(name, param, epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    # save model\n",
    "    torch.save(chessmodel.state_dict(), f'{folder}/models/{epoch}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
