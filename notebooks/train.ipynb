{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import tempfile\n",
    "\n",
    "from lib.service import SamplesService\n",
    "from lib.model import NnueModel\n",
    "from lib.model import decode_int64_bitset\n",
    "from lib.serialize import NnueWriter\n",
    "from lib.puzzles import PuzzleAccuracy\n",
    "from lib.losses import EvalLoss, PQRLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlomb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/notebooks/wandb/run-20240330_033048-s0fidvj3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlomb/cs-master-thesis/runs/s0fidvj3/workspace' target=\"_blank\">20240330_033047_eval_half-piece_4096</a></strong> to <a href='https://wandb.ai/mlomb/cs-master-thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlomb/cs-master-thesis' target=\"_blank\">https://wandb.ai/mlomb/cs-master-thesis</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlomb/cs-master-thesis/runs/s0fidvj3/workspace' target=\"_blank\">https://wandb.ai/mlomb/cs-master-thesis/runs/s0fidvj3/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1000/1000 [00:12<00:00, 77.59it/s]\n",
      " 48%|████▊     | 2333/4897 [00:41<00:49, 51.60it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100000\n",
    "BATCHES_PER_EPOCH = 1000\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "FEATURE_SET = \"half-piece\"\n",
    "NUM_FEATURES = 768 # 192 768 40960\n",
    "METHOD = \"eval\"\n",
    "\n",
    "if METHOD == \"pqr\":\n",
    "    X_SHAPE = (BATCH_SIZE, 3, 2, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 0)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/pqr-1700/*.csv\")\n",
    "    loss_fn = PQRLoss()\n",
    "elif METHOD == \"eval\":\n",
    "    X_SHAPE = (BATCH_SIZE, 2, NUM_FEATURES // 64)\n",
    "    Y_SHAPE = (BATCH_SIZE, 1)\n",
    "    INPUTS = glob(\"/mnt/d/datasets/eval/*.csv\")\n",
    "    loss_fn = EvalLoss()\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "run_name = f'{timestamp}_{METHOD}_{FEATURE_SET}_{BATCH_SIZE}'\n",
    "run = wandb.init(\n",
    "    project=\"cs-master-thesis\",\n",
    "    name=run_name,\n",
    "    job_type=\"train\",\n",
    "    config={\n",
    "        \"feature_set\": FEATURE_SET,\n",
    "        \"method\": METHOD,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"batches_per_epoch\": BATCHES_PER_EPOCH,\n",
    "    }\n",
    ")\n",
    "\n",
    "puzzles = PuzzleAccuracy('./data/puzzles.csv')\n",
    "samples_service = SamplesService(x_shape=X_SHAPE, y_shape=Y_SHAPE, inputs=INPUTS, feature_set=FEATURE_SET, method=METHOD)\n",
    "chessmodel = NnueModel(num_features=NUM_FEATURES)\n",
    "chessmodel.cuda()\n",
    "\n",
    "#for i in tqdm(range(1000000)):\n",
    "#    a = samples_service.next_batch()\n",
    "\n",
    "optimizer = torch.optim.Adam(chessmodel.parameters(), lr=0.0015)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold=0.0001, factor=0.7, patience=50)\n",
    "\n",
    "# @torch.compile # 30% speedup\n",
    "def train_step(X, y):\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = chessmodel(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(outputs, y)\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    chessmodel.clip_weights()\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Make sure gradient tracking is on\n",
    "chessmodel.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    for _ in tqdm(range(BATCHES_PER_EPOCH), desc=f'Epoch {epoch}'):\n",
    "        X, y = samples_service.next_batch()\n",
    "    \n",
    "        # expand bitset\n",
    "        X = decode_int64_bitset(X)\n",
    "        X = X.reshape(-1, 2, NUM_FEATURES)\n",
    "\n",
    "        loss = train_step(X, y)\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        if math.isnan(avg_loss):\n",
    "            raise Exception(\"Loss is NaN, exiting\")\n",
    "\n",
    "    avg_loss /= BATCHES_PER_EPOCH\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(avg_loss)\n",
    "\n",
    "    # log metrics to W&B\n",
    "    wandb.log(step=epoch, data={\n",
    "        \"Train/loss\": avg_loss,\n",
    "        \"Train/lr\": scheduler._last_lr[0], # get_last_lr()\n",
    "\n",
    "        \"Weight/mean-f1\": torch.mean(chessmodel.ft.weight),\n",
    "        \"Weight/mean-l1\": torch.mean(chessmodel.linear1.weight),\n",
    "        \"Weight/mean-l2\": torch.mean(chessmodel.linear2.weight),\n",
    "        \"Weight/mean-out\": torch.mean(chessmodel.output.weight),\n",
    "    })\n",
    "\n",
    "    # save model\n",
    "    with tempfile.NamedTemporaryFile() as tmp:\n",
    "        tmp.write(NnueWriter(chessmodel, FEATURE_SET).buf)\n",
    "    \n",
    "        # store artifact in W&B\n",
    "        artifact = wandb.Artifact(run_name, type=\"model\")\n",
    "        artifact.add_file(tmp.name, name=f\"{epoch}.nn\")\n",
    "        wandb.log_artifact(artifact, aliases=[\"latest\", f\"epoch_{epoch}\"])\n",
    "\n",
    "        if epoch % 30 == 0:\n",
    "            # run puzzles\n",
    "            puzzles_results, puzzles_accuracy = puzzles.measure([\"/mnt/c/Users/mlomb/Desktop/Tesis/cs-master-thesis/engine/target/release/engine\", f\"--nn={tmp.name}\"])\n",
    "\n",
    "            wandb.log(step=epoch, data={\"Puzzles/accuracy\": puzzles_accuracy})\n",
    "            for category, accuracy in puzzles_results:\n",
    "                wandb.log(step=epoch, data={\n",
    "                    f\"Puzzles/{category}\": accuracy\n",
    "                })\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
